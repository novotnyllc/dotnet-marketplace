{
  "created_at": "2026-02-25T03:49:46.951710Z",
  "epic": {
    "data": {
      "branch_name": "fn-60-run-eval-suite-against-skills-and-fix",
      "completion_review_status": "unknown",
      "completion_reviewed_at": null,
      "created_at": "2026-02-24T01:34:21.523619Z",
      "default_impl": null,
      "default_review": null,
      "default_sync": null,
      "depends_on_epics": [],
      "id": "fn-60-run-eval-suite-against-skills-and-fix",
      "next_task": 8,
      "plan_review_status": "unknown",
      "plan_reviewed_at": "2026-02-25T03:46:53.187062Z",
      "spec_path": ".flow/specs/fn-60-run-eval-suite-against-skills-and-fix.md",
      "status": "open",
      "title": "Run Eval Suite Against Skills and Fix to Quality Bar",
      "updated_at": "2026-02-25T03:46:53.193917Z"
    },
    "spec": "# Run Eval Suite Against Skills and Fix to Quality Bar\n\n## Overview\n\nEpic fn-58 built a comprehensive offline evaluation framework with 4 runners (activation, confusion, effectiveness, size impact), 12 rubrics, 73 activation cases, 36 confusion cases, and 11 size impact candidates. Tasks .7/.8/.9 migrated to CLI-based invocations, fixed probe reliability, and added fail-fast error classification.\n\nThis epic closes the loop: analyze eval results, fix skill descriptions and content in small batches with verification at each step, and save baselines for regression tracking.\n\n**Core principle: NEVER run all skills blindly.** Work progressively:\n1. Smoke test the pipeline (2 cases per runner)\n2. Analyze existing results or run targeted diagnostics\n3. Fix skills in batches of 3-5, verifying each batch\n4. Save baselines from verified results\n\n## Approach: Progressive Batched Evaluation\n\n### Phase 1: Infrastructure + Smoke (Task .1)\nAdd `--limit N` flag to all 4 runners so case volume can be capped. Smoke test with `--limit 2` to verify the pipeline works end-to-end after .7/.8/.9 fixes.\n\n### Phase 2: Analyze + Triage (Task .2)\nCheck existing result files in `tests/evals/results/`. If valid post-.7 results exist, analyze them directly (no new CLI calls). If not, run quick targeted diagnostics (`--limit 20` activation, `--limit 3` confusion groups). Produce a prioritized triage of failing skills.\n\n### Phase 3: Fix Routing in Batches (Task .3)\nFix skill descriptions (frontmatter) for activation/confusion issues. Work in batches of 3-5 skills. After each batch: targeted `--skill X` re-run to verify, `validate-skills.sh` to check budget/similarity, commit.\n\n### Phase 4: Fix Content in Batches (Task .4)\nFix skill body content for effectiveness issues. Work in batches of 3-4 skills. Use `--skill X --runs 1 --regenerate` to verify. Run L6 size impact on flagged candidates.\n\n### Phase 5: Quality Bar Verification (Task .5)\nTargeted regression check on all fixed skills. Broader sample on L3/L4 (`--limit 30-40`). Verify quality bar thresholds are met.\n\n### Phase 6: Save Baselines (Task .6)\nSave verified results as baselines. Confirm `compare_baseline.py` compatibility. Unblock fn-58.4.\n\n## Quality Bar Thresholds\n\n### L3 Activation\n- TPR >= 75%, FPR <= 20%, Accuracy >= 70%\n\n### L4 Confusion\n- Per-group accuracy >= 60%, Cross-activation <= 35%, No never-activated skills, Negative control pass rate >= 70%\n\n### L5 Effectiveness\n- Overall win rate >= 50%, Mean improvement > 0, No 0% win rate without documented variance exception\n\n### L6 Size Impact\n- full > baseline in >= 55% of full_vs_baseline comparisons, No skill where baseline consistently beats full\n\n## Quick Commands\n\n```bash\n# Smoke test (2 cases per runner)\npython3 tests/evals/run_activation.py --limit 2\npython3 tests/evals/run_confusion_matrix.py --limit 2\npython3 tests/evals/run_effectiveness.py --limit 2 --runs 1\npython3 tests/evals/run_size_impact.py --limit 2 --runs 1\n\n# Targeted single-skill eval\npython3 tests/evals/run_activation.py --skill dotnet-xunit\npython3 tests/evals/run_effectiveness.py --skill dotnet-xunit --runs 1\n\n# Targeted confusion group\npython3 tests/evals/run_confusion_matrix.py --group testing\n\n# Dry-run (no CLI calls)\npython3 tests/evals/run_activation.py --dry-run\n\n# Validate skills after edits\n./scripts/validate-skills.sh && ./scripts/validate-marketplace.sh\n```\n\n## Scope\n\n**In scope:**\n- Adding --limit flag to eval runners for batch control\n- Analyzing eval results to identify failing skills\n- Fixing skill descriptions (frontmatter) for routing quality\n- Fixing skill body content for effectiveness\n- Saving baselines for regression tracking\n- Unblocking fn-58.4\n\n**Out of scope:**\n- Adding new rubrics, test cases, or eval types\n- CI workflow setup (fn-58.4)\n- Full-suite unattended runs (use targeted batches instead)\n\n## Acceptance\n\n- [ ] All 4 runners support `--limit N` flag\n- [ ] Pipeline validated via smoke test (--limit 2)\n- [ ] Eval results analyzed and failing skills identified\n- [ ] Skill descriptions fixed where routing results indicated problems\n- [ ] Skill content improved where effectiveness showed regression vs baseline\n- [ ] Quality bar thresholds met (or documented rationale for exceptions)\n- [ ] Baselines saved to `tests/evals/baselines/`\n- [ ] `./scripts/validate-skills.sh && ./scripts/validate-marketplace.sh` pass\n- [ ] fn-58.4 unblocked\n\n## Risks\n\n| Risk | Mitigation |\n|------|------------|\n| Existing results invalid after .7-.9 changes | Smoke test validates pipeline; re-run targeted diagnostics if needed |\n| Description edits break other skills | validate-skills.sh after each batch; targeted re-runs |\n| --limit produces unrepresentative samples | Proportional sampling for activation; per-entity semantics for others |\n| Quality bar unachievable with haiku | Thresholds designed for haiku; documented exceptions allowed |\n| Body edits invalidate generation cache | Use --regenerate flag after content changes |\n"
  },
  "epic_id": "fn-60-run-eval-suite-against-skills-and-fix",
  "schema_version": 2,
  "tasks": [
    {
      "data": {
        "created_at": "2026-02-24T01:34:34.323597Z",
        "depends_on": [
          "fn-60-run-eval-suite-against-skills-and-fix.7",
          "fn-60-run-eval-suite-against-skills-and-fix.8",
          "fn-60-run-eval-suite-against-skills-and-fix.9"
        ],
        "epic": "fn-60-run-eval-suite-against-skills-and-fix",
        "id": "fn-60-run-eval-suite-against-skills-and-fix.1",
        "priority": null,
        "spec_path": ".flow/tasks/fn-60-run-eval-suite-against-skills-and-fix.1.md",
        "status": "todo",
        "title": "Add --limit flag to eval runners and smoke test pipeline",
        "updated_at": "2026-02-25T02:17:59.688609Z"
      },
      "id": "fn-60-run-eval-suite-against-skills-and-fix.1",
      "runtime": {
        "status": "todo",
        "updated_at": "2026-02-25T01:56:26.932240Z"
      },
      "spec": "# fn-60.1 Add --limit flag to eval runners and smoke test pipeline\n\n## Description\n\nAdd a `--limit N` argparse flag to all 4 eval runners so case volume can be capped for safe progressive evaluation. Then smoke test the pipeline with `--limit 2` to verify everything works after the .7/.8/.9 CLI migration.\n\n**Depends on:** fn-60.7, fn-60.8, fn-60.9\n**Size:** M\n**Files:**\n- `tests/evals/run_activation.py` (add --limit, proportional sampling)\n- `tests/evals/run_confusion_matrix.py` (add --limit, per-group semantics)\n- `tests/evals/run_effectiveness.py` (add --limit, per-skill semantics)\n- `tests/evals/run_size_impact.py` (add --limit, per-candidate semantics)\n- `tests/evals/run_suite.sh` (pass --limit through to runners)\n- `tests/evals/_common.py` (optional: shared limit helper)\n\n## Approach\n\n### --limit semantics per runner\n\nEach runner interprets `--limit N` based on its primary iteration unit:\n\n- **Activation**: N total cases, but use proportional sampling (shuffle with seeded RNG, then slice) so the limited set contains both positive and negative cases. Follow the openai/evals pattern: `random.Random(seed).shuffle(indices); indices = indices[:limit]`. Without this, `--limit 5` would get 5 core_skills and 0 negatives (files load alphabetically: core_skills, negative_controls, specialized_skills).\n- **Confusion**: N groups (each group retains all its cases). With 7 groups, `--limit 2` runs 2 complete groups. Negative controls are included proportionally (e.g., `--limit 2` includes `ceil(18 * 2/7)` negative controls).\n- **Effectiveness**: N skills (each skill retains all prompts \u00d7 runs). With 12 rubric'd skills, `--limit 3` evaluates 3 complete skills.\n- **Size impact**: N candidates (each candidate retains all comparison types \u00d7 runs). With 11 candidates, `--limit 3` evaluates 3 complete candidates.\n\n### Result metadata\n\nWhen `--limit` is used, record `meta.limit: N` in the result JSON so reviewers know a result came from a partial run. Follow pattern at `_common.py:build_run_metadata()`.\n\n### Validation\n\n- `--limit 0` and negative values: reject with argparse error\n- `--limit` exceeds dataset size: silently cap (follow lm-eval-harness)\n- `--limit` combined with `--skill`/`--group`: `--skill` filters first, then `--limit` caps the filtered set\n- Warn when `--limit` is used: print `\"WARNING: --limit is for development/testing. Full-dataset runs needed for baselines.\"` to stderr (follow lm-eval-harness)\n\n### run_suite.sh update\n\nAdd `--limit=N` passthrough: `run_suite.sh --limit=5` passes `--limit 5` to each runner invocation.\n\n### Smoke test\n\nAfter implementing `--limit`, run each runner with `--limit 2 --runs 1` to confirm end-to-end pipeline works:\n```\npython3 tests/evals/run_activation.py --limit 2\npython3 tests/evals/run_confusion_matrix.py --limit 2\npython3 tests/evals/run_effectiveness.py --limit 2 --runs 1\npython3 tests/evals/run_size_impact.py --limit 2 --runs 1\n```\n\nEach should complete with exit 0, produce a result JSON, and emit TOTAL_CALLS/COST_USD/ABORTED/N_CASES/FAIL_FAST on stdout.\n\n## Acceptance\n\n- [ ] All 4 runners accept `--limit N` argparse flag (integer > 0)\n- [ ] Activation `--limit` uses proportional sampling (seeded shuffle-then-slice) so limited sets include positive and negative cases\n- [ ] Confusion `--limit N` limits to N groups (not N total cases)\n- [ ] Effectiveness `--limit N` limits to N skills\n- [ ] Size impact `--limit N` limits to N candidates\n- [ ] `--limit` exceeding dataset size silently caps without error\n- [ ] `--limit 0` and negative values produce argparse error\n- [ ] When `--limit` is used, result JSON includes `meta.limit: N`\n- [ ] Warning message printed to stderr when `--limit` is used\n- [ ] `run_suite.sh --limit=N` passes --limit to all 4 runners\n- [ ] Smoke test passes: each runner with `--limit 2 --runs 1` exits 0, produces result JSON, emits stdout keys\n- [ ] `--dry-run` still works (no regression)\n- [ ] `--limit` combined with `--skill`/`--group` works (filter first, then cap)\n- [ ] `./scripts/validate-skills.sh && ./scripts/validate-marketplace.sh` pass (no skill changes yet)\n\n## Done summary\n\n## Evidence\n- Commits:\n- Tests:\n- PRs:\n"
    },
    {
      "data": {
        "created_at": "2026-02-24T01:34:37.663196Z",
        "depends_on": [
          "fn-60-run-eval-suite-against-skills-and-fix.1"
        ],
        "epic": "fn-60-run-eval-suite-against-skills-and-fix",
        "id": "fn-60-run-eval-suite-against-skills-and-fix.2",
        "priority": null,
        "spec_path": ".flow/tasks/fn-60-run-eval-suite-against-skills-and-fix.2.md",
        "status": "todo",
        "title": "Analyze eval results and triage failing skills",
        "updated_at": "2026-02-25T02:18:00.930632Z"
      },
      "id": "fn-60-run-eval-suite-against-skills-and-fix.2",
      "runtime": {
        "status": "todo",
        "updated_at": "2026-02-24T03:23:46.597971Z"
      },
      "spec": "# fn-60.2 Analyze eval results and triage failing skills\n\n## Description\n\nCheck existing result files from prior runs. If valid post-.7 results exist, analyze them directly without new CLI calls. If not, run targeted diagnostics with `--limit` to get quick signal. Produce a prioritized triage document listing which skills need routing fixes (L3/L4) and which need content fixes (L5/L6).\n\n**Depends on:** fn-60.1\n**Size:** M\n**Files:**\n- `tests/evals/results/` (read existing results)\n- `tests/evals/results/triage.md` (new: triage document)\n- `tests/evals/eval-progress.json` (initialize with triage findings)\n\n## Approach\n\n### Step 1: Inventory existing results\n\nList result files in `tests/evals/results/`. For each eval type, check:\n- Does a non-aborted result exist (`ABORTED=0` in meta or full case count)?\n- Was it produced with the current CLI-based runner (post-.7)?\n- Check `meta.backend` field \u2014 should be `claude` (not `anthropic-sdk` which was pre-.7)\n\n### Step 2: Quick targeted diagnostic (if needed)\n\nIf existing results are missing or pre-.7, run targeted diagnostics:\n- L3 activation: `--limit 20` (~20 CLI calls) for a representative sample\n- L4 confusion: `--limit 3` (3 groups, ~15 CLI calls) for group-level signal\n- L5 effectiveness: `--limit 4 --runs 1` (4 skills, ~24 CLI calls) for quick signal\n- L6 size impact: `--limit 3 --runs 1` (3 candidates, ~9 CLI calls)\n\nTotal worst case: ~68 CLI calls for diagnostic coverage.\n\n### Step 3: Triage analysis\n\nFor each eval type, extract findings:\n\n**L3 Activation** (quality bar: TPR>=75%, FPR<=20%, Accuracy>=70%):\n- List skills with lowest activation rates\n- List cases where wrong skill was activated\n- Identify description patterns causing misrouting\n\n**L4 Confusion** (quality bar: per-group>=60%, no never-activated):\n- List groups below 60% accuracy\n- Identify cross-activation pairs\n- Flag never-activated skills within groups\n\n**L5 Effectiveness** (quality bar: win_rate>=50%, no 0% without exception):\n- List skills with 0% win rate (priority fixes)\n- List skills below 50% win rate\n- Identify common content quality issues\n\n**L6 Size Impact** (quality bar: full>baseline in 55%+):\n- Flag skills where baseline consistently beats full\n- Identify candidates where full content may be harmful\n\n### Step 4: Write triage document\n\nCreate `tests/evals/results/triage.md` with:\n- Summary of current state vs quality bar\n- Priority 1 fixes (blocking quality bar)\n- Priority 2 fixes (improvement opportunities)\n- Specific skills to fix in each category\n- Recommended batch order for .3 and .4\n\n### Step 5: Initialize eval-progress.json\n\nPopulate `tests/evals/eval-progress.json` with an entry per skill from triage findings. Each entry records:\n- `eval_types`: which eval types have been run against this skill (from existing results)\n- `status`: one of `untested`, `passing`, `needs-routing-fix`, `needs-content-fix`, `fixed`, `exception`\n- `agent`: the agent/session ID that last evaluated it (use `RALPH_SESSION_ID` env var if set, else `\"manual\"`)\n- `run_ids`: list of result file run_ids where this skill appeared\n- `fix_task`: which task is responsible for fixing (`.3` for routing, `.4` for content)\n- `notes`: brief description of the issue\n\nThis file is committed with the triage document and read by subsequent tasks (.3, .4, .5) to know what's been done and what remains.\n\n## Key context\n\nExisting results may show: L3 TPR=69%, FPR=28%, Accuracy=70% (below thresholds). L4 mostly passing. L5 has 4 skills at 0% win rate. L6 has 2 candidates where baseline beats full.\n\n## Acceptance\n\n- [ ] Existing result files inventoried (count per eval type, post-.7 validity checked)\n- [ ] If no valid results exist, targeted diagnostics run with --limit (not full suite)\n- [ ] Triage document written to `tests/evals/results/triage.md`\n- [ ] L3 activation: specific failing skills listed with activation rates\n- [ ] L4 confusion: groups below threshold listed, never-activated skills flagged\n- [ ] L5 effectiveness: 0% win rate skills identified, below-50% skills listed\n- [ ] L6 size impact: baseline-beats-full candidates flagged\n- [ ] Priority batches defined for tasks .3 (routing fixes) and .4 (content fixes)\n- [ ] Total CLI calls for this task documented (should be 0 if reusing existing results, ~68 max if diagnostics needed)\n- [ ] `eval-progress.json` populated with per-skill entries from triage findings\n- [ ] `./scripts/validate-skills.sh && ./scripts/validate-marketplace.sh` pass (no skill changes yet)\n\n## Done summary\n\n## Evidence\n- Commits:\n- Tests:\n- PRs:\n"
    },
    {
      "data": {
        "assignee": null,
        "claim_note": "",
        "claimed_at": null,
        "created_at": "2026-02-24T01:34:41.008238Z",
        "depends_on": [
          "fn-60-run-eval-suite-against-skills-and-fix.2"
        ],
        "epic": "fn-60-run-eval-suite-against-skills-and-fix",
        "id": "fn-60-run-eval-suite-against-skills-and-fix.3",
        "priority": null,
        "spec_path": ".flow/tasks/fn-60-run-eval-suite-against-skills-and-fix.3.md",
        "status": "todo",
        "title": "Fix routing descriptions in batches with verification",
        "updated_at": "2026-02-25T02:18:02.073061Z"
      },
      "id": "fn-60-run-eval-suite-against-skills-and-fix.3",
      "runtime": null,
      "spec": "# fn-60.3 Fix routing descriptions in batches with verification\n\n## Description\n\nFix skill descriptions (frontmatter) to improve L3 activation routing and L4 confusion disambiguation. Work in batches of 3-5 skills. After each batch: verify with targeted `--skill` re-runs, run `validate-skills.sh` to check budget/similarity, and commit.\n\n**Depends on:** fn-60.2\n**Size:** M\n**Files:**\n- `skills/*/SKILL.md` (frontmatter description edits only \u2014 NOT body content)\n- `tests/evals/results/` (targeted re-run results)\n- `tests/evals/eval-progress.json` (read for batch selection, update after each batch)\n\n## Approach\n\n### Batch workflow (repeat for each batch of 3-5 skills)\n\n1. **Select batch**: Pick 3-5 skills from the triage priority list (task .2 output)\n2. **Edit descriptions**: Modify the `description:` frontmatter field in each skill's SKILL.md\n   - Follow routing style guide at `docs/skill-routing-style-guide.md`\n   - Action + Domain + Differentiator formula\n   - Keep under 120 chars per description\n3. **Validate**: `./scripts/validate-skills.sh` \u2014 check budget (12K warn / 15.6K fail), similarity, and frontmatter\n4. **Verify with targeted re-runs**:\n   - L3: `python3 tests/evals/run_activation.py --skill <name>` for each edited skill\n   - L4: `python3 tests/evals/run_confusion_matrix.py --group <group>` for affected confusion groups\n5. **Update eval-progress.json**: For each edited skill, set `status` to `fixed`, record `agent` (use `RALPH_SESSION_ID` env var if set, else `\"manual\"`), add the re-run `run_ids`, and note what was changed\n6. **Commit batch**: `git add skills/*/SKILL.md tests/evals/eval-progress.json && git commit -m \"fix(skills): improve routing for <batch summary>\"`\n\n### Constraints\n\n- **Budget neutral**: Total description budget must not increase from pre-fix baseline (target staying under 12,000 chars). Check in validate-skills.sh.\n- **Similarity compliance**: Must not re-introduce similarity violations cleared by fn-53. Check via `scripts/similarity-baseline.json`.\n- **Description only**: Do NOT edit skill body content in this task \u2014 that's task .4. Isolating description vs content changes allows attribution of improvements.\n\n### Expected batch count\n\nBased on typical triage findings: 2-3 batches of 3-5 skills each (6-15 total edits). The exact count depends on task .2 findings.\n\n## Key context\n\n- Confusion runner uses `--group <name>`, not `--skill`. Groups are domain categories (e.g., \"testing\", \"api\", \"security\").\n- Activation `--skill <name>` also includes all 18 negative controls alongside the filtered skill's cases. This is expected \u2014 it tests that the model doesn't falsely activate on negatives.\n- Description edits don't invalidate effectiveness/size_impact generation caches (those keys include body hash, not description).\n\n## Acceptance\n\n- [ ] All priority-1 routing skills from triage (task .2) have been fixed\n- [ ] Each batch verified with targeted `--skill` (activation) and `--group` (confusion) re-runs\n- [ ] `./scripts/validate-skills.sh` passes after each batch (budget + similarity + frontmatter)\n- [ ] `./scripts/validate-marketplace.sh` passes\n- [ ] Total description budget is neutral or reduced (no increase from pre-fix baseline)\n- [ ] Each batch committed separately with descriptive message\n- [ ] Re-run results saved in `tests/evals/results/` with descriptive filenames\n- [ ] Targeted L3 activation re-runs show improvement for edited skills (activation rate up or misrouting resolved)\n- [ ] L4 confusion re-runs show improvement for affected groups (per-group accuracy up or cross-activation down)\n- [ ] `eval-progress.json` updated after each batch with agent ID, run_ids, and status\n\n## Done summary\n\n## Evidence\n- Commits:\n- Tests:\n- PRs:\n"
    },
    {
      "data": {
        "assignee": null,
        "claim_note": "",
        "claimed_at": null,
        "created_at": "2026-02-24T01:34:44.467374Z",
        "depends_on": [
          "fn-60-run-eval-suite-against-skills-and-fix.3"
        ],
        "epic": "fn-60-run-eval-suite-against-skills-and-fix",
        "id": "fn-60-run-eval-suite-against-skills-and-fix.4",
        "priority": null,
        "spec_path": ".flow/tasks/fn-60-run-eval-suite-against-skills-and-fix.4.md",
        "status": "todo",
        "title": "Fix effectiveness content and L6 issues in batches",
        "updated_at": "2026-02-25T02:20:19.273985Z"
      },
      "id": "fn-60-run-eval-suite-against-skills-and-fix.4",
      "runtime": null,
      "spec": "# fn-60.4 Fix effectiveness content and L6 issues in batches\n\n## Description\n\nFix skill body content for skills with poor L5 effectiveness scores. Also address L6 size impact issues where baseline beats full. Work in batches of 3-4 skills. Use `--skill X --runs 1 --regenerate` to verify each batch.\n\n**Depends on:** fn-60.3\n**Size:** M\n**Files:**\n- `skills/*/SKILL.md` (body content edits \u2014 sections below frontmatter)\n- `tests/evals/results/` (targeted re-run results)\n- `tests/evals/eval-progress.json` (read for batch selection, update after each batch)\n\n## Approach\n\n### Effectiveness fix workflow (batches of 3-4 skills)\n\n1. **Select batch**: Pick 3-4 skills from the L5 triage priority list (0% win rate skills first)\n2. **Analyze failure mode**: Read the skill's rubric YAML (`tests/evals/rubrics/<skill>.yaml`) to understand what the eval tests. Read the effectiveness result details to see what the baseline does better.\n3. **Edit content**: Improve the skill body in SKILL.md. Focus on the specific rubric criteria the skill is failing on.\n4. **Verify**: `python3 tests/evals/run_effectiveness.py --skill <name> --runs 1 --regenerate`\n   - MUST use `--regenerate` after body edits (cache keys include body hash; stale cache = false results)\n   - With `--runs 1`: 2 prompts per skill = 6 CLI calls (2 generations + 2 baselines + 2 judge calls)\n5. **Validate**: `./scripts/validate-skills.sh` \u2014 ensure no structural regressions\n6. **Update eval-progress.json**: For each edited skill, set `status` to `fixed`, record `agent` (use `RALPH_SESSION_ID` env var if set, else `\"manual\"`), add the re-run `run_ids`, and note what was changed\n7. **Commit batch**: `git add skills/*/SKILL.md tests/evals/eval-progress.json && git commit -m \"fix(skills): improve effectiveness for <batch summary>\"`\n\n### Size impact fix workflow (if flagged in triage)\n\nFor candidates where baseline consistently beats full (all 3 runs):\n1. Investigate: is the full content introducing noise that confuses the model?\n2. Fix: trim or restructure the body content\n3. Verify: `python3 tests/evals/run_size_impact.py --skill <name> --runs 1 --regenerate`\n\n### Expected scope\n\n- L5 priority: ~4 skills at 0% win rate, ~3-4 more below 50% = 2-3 batches\n- L6 priority: ~2 candidates flagged = 1 batch\n- Total CLI calls: ~60-90 (6 calls per skill \u00d7 10-15 skills)\n\n## Key context\n\n- **MUST use --regenerate after body edits** \u2014 effectiveness and size_impact cache generations keyed by body hash. Without --regenerate, stale cached outputs produce meaningless comparisons. (Memory pitfall: generation cache keys must include ALL inputs that affect output.)\n- Description edits from task .3 do NOT invalidate these caches (description is not in the hash). Only body content changes require --regenerate.\n- L5 quality bar: no skill at 0% win rate (unless documented variance exception with n=6 cases). L6: no candidate where baseline sweeps all runs.\n\n## Acceptance\n\n- [ ] All 0% win rate skills from triage (task .2) have been fixed or documented as variance exceptions\n- [ ] Each batch verified with `--skill X --runs 1 --regenerate` (effectiveness)\n- [ ] L6 flagged candidates addressed (no baseline sweep remaining, or documented rationale)\n- [ ] `--regenerate` used on every re-run after body edits (no stale cache results)\n- [ ] `./scripts/validate-skills.sh && ./scripts/validate-marketplace.sh` pass\n- [ ] Each batch committed separately with descriptive message\n- [ ] Re-run results saved in `tests/evals/results/`\n- [ ] No NEW 0% win rate skills introduced by the changes\n- [ ] `eval-progress.json` updated after each batch with agent ID, run_ids, and status\n\n## Done summary\n\n## Evidence\n- Commits:\n- Tests:\n- PRs:\n"
    },
    {
      "data": {
        "assignee": null,
        "claim_note": "",
        "claimed_at": null,
        "created_at": "2026-02-24T01:34:48.403802Z",
        "depends_on": [
          "fn-60-run-eval-suite-against-skills-and-fix.3",
          "fn-60-run-eval-suite-against-skills-and-fix.4"
        ],
        "epic": "fn-60-run-eval-suite-against-skills-and-fix",
        "id": "fn-60-run-eval-suite-against-skills-and-fix.5",
        "priority": null,
        "spec_path": ".flow/tasks/fn-60-run-eval-suite-against-skills-and-fix.5.md",
        "status": "todo",
        "title": "Quality bar verification sweep",
        "updated_at": "2026-02-25T02:18:04.068981Z"
      },
      "id": "fn-60-run-eval-suite-against-skills-and-fix.5",
      "runtime": null,
      "spec": "# fn-60.5 Quality bar verification sweep\n\n## Description\n\nRun targeted regression checks on all skills that were fixed in .3/.4, plus a broader sample across L3/L4 to verify quality bar thresholds are met. This is NOT a blind full-suite run \u2014 it's a focused verification of fixes plus a representative sample for confidence.\n\n**Depends on:** fn-60.3, fn-60.4\n**Size:** M\n**Files:**\n- `tests/evals/results/` (verification results)\n- `tests/evals/eval-progress.json` (read to identify which skills were fixed in .3/.4, update with verification results)\n\n## Approach\n\n### Step 1: Targeted re-verification of all fixed skills\n\nRead `tests/evals/eval-progress.json` to get the list of skills with `status: \"fixed\"` from .3 and .4. Re-run each eval type on those skills/groups:\n- L3 activation: `--skill <name>` for each skill edited in .3 (confirm routing improvements held)\n- L4 confusion: `--group <group>` for each group affected in .3\n- L5 effectiveness: `--skill <name> --runs 3 --regenerate` for each skill edited in .4 (multi-run for statistical confidence)\n- L6 size impact: `--skill <name> --runs 3 --regenerate` for each candidate addressed in .4\n\n### Step 2: Broader representative sample\n\nRun a broader sample to check for regressions in un-edited skills:\n- L3 activation: `--limit 40` (~40 proportionally sampled cases) \u2014 covers ~55% of cases\n- L4 confusion: `--limit 5` (5 of 7 groups) \u2014 covers ~71% of groups\n\nThis provides representative coverage without running the full dataset. If the sample meets thresholds, the full dataset almost certainly does too.\n\n### Step 3: Quality bar check\n\nVerify thresholds against the sample results:\n- L3: TPR >= 75%, FPR <= 20%, Accuracy >= 70%\n- L4: Per-group accuracy >= 60%, negative controls >= 70%\n- L5: Overall win rate >= 50%, no 0% without exception\n- L6: full > baseline in >= 55%, no baseline sweep\n\nIf any threshold is missed on the sample: investigate which specific cases fail, determine if they're fixable, and either fix + re-verify or document as exceptions.\n\n### CLI call estimate\n\n- Fixed skills re-runs: ~50-80 calls (depends on .3/.4 scope)\n- L3 sample: ~40 calls\n- L4 sample: ~25 calls\n- Total: ~115-145 calls (spread across targeted and sample runs)\n\n## Acceptance\n\n- [ ] All skills edited in .3 re-verified with targeted activation/confusion re-runs\n- [ ] All skills edited in .4 re-verified with `--runs 3 --regenerate` for statistical confidence\n- [ ] Broader L3 sample (`--limit 40`) meets quality bar: TPR >= 75%, FPR <= 20%, Accuracy >= 70%\n- [ ] Broader L4 sample (`--limit 5`) meets quality bar: per-group >= 60%, negative controls >= 70%\n- [ ] L5 overall win rate >= 50% (no 0% win rate without documented exception)\n- [ ] L6 no baseline sweep remaining (or documented rationale)\n- [ ] Any threshold misses documented with specific failing cases and rationale\n- [ ] `./scripts/validate-skills.sh && ./scripts/validate-marketplace.sh` pass\n- [ ] Results saved in `tests/evals/results/`\n- [ ] `eval-progress.json` updated with verification results and final status per skill\n\n## Done summary\n\n## Evidence\n- Commits:\n- Tests:\n- PRs:\n"
    },
    {
      "data": {
        "assignee": null,
        "claim_note": "",
        "claimed_at": null,
        "created_at": "2026-02-24T01:34:51.977858Z",
        "depends_on": [
          "fn-60-run-eval-suite-against-skills-and-fix.5"
        ],
        "epic": "fn-60-run-eval-suite-against-skills-and-fix",
        "id": "fn-60-run-eval-suite-against-skills-and-fix.6",
        "priority": null,
        "spec_path": ".flow/tasks/fn-60-run-eval-suite-against-skills-and-fix.6.md",
        "status": "todo",
        "title": "Save baselines and unblock fn-58.4",
        "updated_at": "2026-02-25T02:18:05.155823Z"
      },
      "id": "fn-60-run-eval-suite-against-skills-and-fix.6",
      "runtime": null,
      "spec": "# fn-60.6 Save baselines and unblock fn-58.4\n\n## Description\n\nSave verified eval results as baseline files for future regression tracking. Verify `compare_baseline.py` works against them. Unblock fn-58.4 (CI workflow).\n\n**Depends on:** fn-60.5\n**Size:** S\n**Files:**\n- `tests/evals/baselines/activation_baseline.json` (new)\n- `tests/evals/baselines/confusion_baseline.json` (new)\n- `tests/evals/baselines/effectiveness_baseline.json` (new)\n- `tests/evals/baselines/size_impact_baseline.json` (new)\n- `tests/evals/compare_baseline.py` (read-only verification)\n\n## Approach\n\n### Step 1: Select best result files\n\nFor each eval type, select the most representative result file from `tests/evals/results/`:\n- Prefer the latest non-aborted, non-limited result (full coverage if available from .5)\n- If only limited/targeted results exist, use the broadest one and document the coverage in the baseline metadata\n- The result file's `summary` schema must be stable (compare_baseline.py compatibility)\n\n### Step 2: Copy to baselines directory\n\n```\ncp tests/evals/results/<best_activation>.json tests/evals/baselines/activation_baseline.json\ncp tests/evals/results/<best_confusion>.json tests/evals/baselines/confusion_baseline.json\ncp tests/evals/results/<best_effectiveness>.json tests/evals/baselines/effectiveness_baseline.json\ncp tests/evals/results/<best_size_impact>.json tests/evals/baselines/size_impact_baseline.json\n```\n\n### Step 3: Verify compare_baseline.py\n\nRun `compare_baseline.py` against each baseline to confirm it:\n- Loads the baseline file successfully\n- Parses the summary schema\n- Produces a comparison output (even if \"no previous baseline\" for first run)\n\n### Step 4: Commit and document\n\n- Commit baseline files\n- Note in commit message which result files were used as sources\n- Document baseline coverage (full dataset vs partial) in commit message\n\n## Key context\n\n- compare_baseline.py uses \"latest result file by mtime\" for comparison, and loads baselines from the configured baselines directory\n- fn-58.4 depends on these baseline files existing to set up CI regression gates\n- Memory decision: \"CI baseline regression gates must handle schema evolution: new entries absent from the baseline should be treated as 'new coverage', not hard failures\"\n\n## Acceptance\n\n- [ ] All 4 baseline files exist in `tests/evals/baselines/`\n- [ ] `compare_baseline.py` loads each baseline without error\n- [ ] `compare_baseline.py` produces comparison output for each eval type\n- [ ] Baseline files committed with documentation of source result files\n- [ ] `./scripts/validate-skills.sh && ./scripts/validate-marketplace.sh` pass\n- [ ] fn-58.4 dependency is satisfied (baseline files in expected location with expected schema)\n\n## Done summary\n\n## Evidence\n- Commits:\n- Tests:\n- PRs:\n"
    },
    {
      "data": {
        "created_at": "2026-02-24T01:42:09.523578Z",
        "depends_on": [],
        "epic": "fn-60-run-eval-suite-against-skills-and-fix",
        "id": "fn-60-run-eval-suite-against-skills-and-fix.7",
        "priority": 0,
        "spec_path": ".flow/tasks/fn-60-run-eval-suite-against-skills-and-fix.7.md",
        "status": "todo",
        "title": "Replace SDK API layer with CLI-based invocations",
        "updated_at": "2026-02-24T03:59:46.775007Z"
      },
      "id": "fn-60-run-eval-suite-against-skills-and-fix.7",
      "runtime": {
        "assignee": "claire@novotny.org",
        "claimed_at": "2026-02-24T04:15:28.152825Z",
        "evidence": {
          "commits": [
            "562a7b0ccc5a54cab643b8c1d9bb707b6b2882d6",
            "bbbfcbc",
            "adf46c9",
            "0891e39",
            "820fb80",
            "4576405",
            "fc99de1",
            "9843bdd"
          ],
          "prs": [],
          "tests": [
            "python3 tests/evals/run_activation.py --dry-run",
            "python3 tests/evals/run_confusion_matrix.py --group testing --dry-run",
            "python3 tests/evals/run_effectiveness.py --dry-run",
            "python3 tests/evals/run_size_impact.py --dry-run",
            "./scripts/validate-skills.sh",
            "./scripts/validate-marketplace.sh",
            "grep -r client.messages.create tests/evals/",
            "grep -r 'import anthropic' tests/evals/",
            "grep -r ANTHROPIC_API_KEY tests/evals/"
          ]
        },
        "status": "done",
        "updated_at": "2026-02-24T05:43:38.745167Z"
      },
      "spec": "# fn-60.7 Replace SDK API layer with CLI-based invocations\n\n## Description\n\nThe eval runners currently use the Anthropic Python SDK (`anthropic.Anthropic().messages.create(...)`) for all LLM calls. This requires `ANTHROPIC_API_KEY` and does NOT reflect what actual coding clients do. The CLI clients (`claude`, `codex`, `copilot`) are already authenticated locally -- use them directly via subprocess.\n\nThis task replaces the entire SDK-based API layer with CLI subprocess invocations. All 4 runners and the shared judge module must be updated.\n\n**Depends on:** (none -- this is the first task to execute)\n**Size:** L\n**Files:**\n- `tests/evals/_common.py` -- replace `get_client()` with `call_model()` CLI wrapper; remove `anthropic` import\n- `tests/evals/run_activation.py` -- replace `client.messages.create()` calls with `call_model()`; add `--cli` flag\n- `tests/evals/run_confusion_matrix.py` -- replace `client.messages.create()` calls with `call_model()`; add `--cli` flag\n- `tests/evals/run_effectiveness.py` -- replace `client.messages.create()` calls with `call_model()`; add `--cli` flag\n- `tests/evals/run_size_impact.py` -- replace `client.messages.create()` calls with `call_model()`; add `--cli` flag\n- `tests/evals/judge_prompt.py` -- replace `client.messages.create()` in `invoke_judge()` with `call_model()`\n- `tests/evals/config.yaml` -- add `cli` section with per-backend model names; add `cost.max_calls_per_run`; remove `ANTHROPIC_API_KEY` references\n- `tests/evals/requirements.txt` -- remove `anthropic` (keep `pyyaml`)\n- `tests/evals/run_suite.sh` -- remove `ANTHROPIC_API_KEY` check; parse stable machine-parseable keys instead of regexing prose\n\n## Approach\n\n### New `call_model()` function in `_common.py`\n\nReplace `get_client()` with a `call_model()` function:\n\n```python\ndef call_model(\n    system_prompt: str,\n    user_prompt: str,\n    model: Optional[str] = None,\n    max_tokens: int = 4096,\n    temperature: float = 0.0,\n    cli: Optional[str] = None,\n) -> dict:\n    \"\"\"Call an LLM via CLI subprocess.\n\n    Returns:\n        {\"text\": str, \"cost\": float, \"input_tokens\": int, \"output_tokens\": int, \"calls\": 1}\n        cost/token fields are 0 if not available from the CLI output.\n        calls is always 1 (used for call-count-based abort logic).\n    \"\"\"\n```\n\n### CLI capability detection\n\nOn first invocation per backend, `call_model()` performs a lightweight capability check:\n1. Verify the configured CLI tool is in PATH (`shutil.which()`)\n2. Probe capabilities with a trivial prompt for **each backend** (not just claude):\n   - **`claude`**: probe `--output-format json` and stdin piping (`-p` without inline arg, prompt via stdin)\n   - **`codex`**: probe stdin piping (prompt via stdin to `-q -`)\n   - **`copilot`**: probe stdin piping (prompt via stdin to `-p -`)\n   - For each backend, determine `prompt_mode`: stdin > file > arg (try in order, cache first working mode)\n3. Cache results in a module-level `_cli_caps` dict so detection runs only once per process per backend. Cache keys: `{cli_name}.json_output`, `{cli_name}.prompt_mode` (stdin | file | arg).\n\nIf detection fails:\n- Emit a one-time actionable diagnostic to stderr with the specific failing capability and remediation steps (e.g., \"WARNING: claude CLI does not support --output-format json. Falling back to text mode. Upgrade claude or set cli.default: codex in config.yaml\")\n- Fall back gracefully: text-only mode (no JSON parsing, cost/usage = 0), arg-based prompt passing\n\n### Implementation per CLI tool\n\n**System prompt transport**: L3/L4 system prompts embed the routing index (~10-15k chars), which exceeds safe CLI argument limits. Strategy:\n- When `len(system_prompt) <= 4000`: pass as `--system-prompt \"...\"` CLI argument (optimization for short prompts)\n- When `len(system_prompt) > 4000`: combine into a single stdin payload: `combined = system_prompt + \"\\n\\n---\\n\\n\" + user_prompt`, omit `--system-prompt` flag\n- This applies uniformly to ALL backends (claude, codex, copilot)\n\n**`claude` (primary)**:\n```bash\n# Short system prompt:\nclaude -p --system-prompt \"SYSTEM_PROMPT\" --model MODEL \\\n  --output-format json --tools \"\" --no-session-persistence \\\n  --disable-slash-commands --max-turns 1\n# Long system prompt (>4k chars): omit --system-prompt, combine into stdin\nclaude -p --model MODEL --output-format json --tools \"\" \\\n  --no-session-persistence --disable-slash-commands --max-turns 1\n```\n- Pass prompt (user or combined) via **stdin pipe** (`input=prompt` in `subprocess.run()`) -- preferred mode if capability detection confirms stdin works\n- Fall back to temp-file-fed-stdin if direct stdin piping is not supported (see capability detection)\n- `--tools \"\"` disables tool use (pure text generation)\n- `--output-format json` returns structured JSON with response and usage\n- `--max-turns 1` prevents multi-turn loops\n- `--no-session-persistence` prevents session state from accumulating\n- `--disable-slash-commands` prevents skill loading during eval\n- Parse response text from JSON output using multi-shape detection:\n  - Shape A: `{\"result\": \"...\", \"usage\": {...}}` (current claude CLI format)\n  - Shape B: `{\"content\": [{\"text\": \"...\"}], \"usage\": {...}}` (API-like format)\n  - Shape C: `{\"completion\": \"...\"}` (legacy format)\n  - Unknown shape: fall back to **raw stdout as text**, emit one-time warning with top-level keys encountered (e.g., \"WARNING: Unknown claude JSON shape, keys: ['foo', 'bar']. Using raw output as text.\")\n- Extract usage/cost fields from whichever shape matches; default to 0 if not found\n- Fallback: if JSON mode unavailable, capture stdout as plain text\n\n**`codex`**:\n```bash\necho \"SYSTEM_CONTEXT\\n\\nUSER_PROMPT\" | codex --approval-mode full-auto -m MODEL -q -\n```\n- No `--system-prompt` flag -- prepend system prompt to user message\n- Model names differ (e.g., `o3`, `o4-mini`)\n- Prompt transport: prefer **stdin pipe** (same as claude) to avoid arg length limits; fall back to temp file if stdin not accepted. Capability detection probes codex stdin support on first call.\n\n**`copilot`**:\n```bash\necho \"SYSTEM_CONTEXT\\n\\nUSER_PROMPT\" | copilot -p -\n```\n- No `--system-prompt` flag -- prepend system prompt to user message\n- Uses its own model selection\n- Prompt transport: prefer **stdin pipe**; fall back to temp file. Capability detection probes copilot stdin support on first call.\n\n### Config changes\n\n```yaml\n# config.yaml\ncli:\n  default: claude           # which CLI tool to use (claude | codex | copilot)\n  claude:\n    model: haiku            # CLI-native model string (not SDK model ID)\n  codex:\n    model: o4-mini          # codex model name\n  copilot:\n    model: null             # copilot picks its own model\n\ncost:\n  max_cost_per_run: 5.00    # dollar-based cap (effective when CLI reports cost)\n  max_calls_per_run: 500    # call-count cap (always effective; safety net when cost unavailable)\n```\n\n**Config contract**: `cli.default` selects the backend. Per-backend `model` is a CLI-native string. `build_run_metadata()` stores `cli.default` as `backend` and the resolved model string as `model` in result envelopes. This keeps metadata meaningful and stable for baselines/compare.\n\n### Runner changes\n\nEach runner's `main()` function currently does:\n```python\nclient = _common.get_client()\n# ... then later:\nresponse = client.messages.create(model=..., system=..., messages=[...])\ntext = response.content[0].text\n```\n\nReplace with:\n```python\nresult = _common.call_model(system_prompt=..., user_prompt=..., model=...)\ntext = result[\"text\"]\n```\n\nAll `client.messages.create()` call sites across the 4 runners and judge_prompt.py must be updated. Remove the `client` variable entirely. Verify completeness via grep: no remaining `client.messages.create` or `anthropic` imports anywhere under `tests/evals/`.\n\n### `--cli` flag on all runners\n\nAll 4 runners get a new `--cli {claude,codex,copilot}` argparse flag that overrides `config.yaml:cli.default` for that run. The flag is passed through to `call_model()`.\n\n### run_suite.sh changes\n\n- Remove the `ANTHROPIC_API_KEY` check block (lines 47-53)\n- Remove `ANTHROPIC_API_KEY` from the usage comment\n- Parse stable machine-parseable keys from runner stdout: `TOTAL_CALLS=`, `COST_USD=`, `ABORTED=`, `N_CASES=`\n- Replace prose-regex cost extraction with key-based parsing\n\n### Runner output contract\n\nEach runner prints these keys to **stdout** (not stderr) as the last output before exiting:\n```\nTOTAL_CALLS=<int>\nCOST_USD=<float>\nABORTED=0|1\nN_CASES=<int>\n```\n\nAll other runner output (progress, diagnostics, per-skill details) goes to **stderr**. `run_suite.sh` captures stdout to parse the key-value lines and captures stderr separately for logging. This avoids mixing parseable keys with prose output.\n\n### Dual abort mechanism\n\nRunners enforce **(cost OR call-count) caps**, whichever triggers first:\n- `call_model()` returns `calls=1`; runners increment a shared `total_calls` counter\n- Before each call: check `total_calls >= max_calls_per_run` OR `total_cost >= max_cost_per_run`\n- If either triggers: write partial results, print `ABORTED=1`, exit 0\n\n### Prompt passing strategy (uniform across backends)\n\nLarge prompts (routing indices, skill bodies) can exceed shell argument limits. `call_model()` uses a **uniform prompt transport strategy** across ALL backends:\n\n1. **Prefer stdin pipe** (`input=combined_prompt` in `subprocess.run()`) -- probed per-backend during capability detection\n2. **Fall back to temp-file-fed-stdin** if direct stdin piping fails -- write prompt to `tempfile.NamedTemporaryFile`, then `open()` and pass file object as `stdin` to `subprocess.run()`. This is internal plumbing; the CLI still sees stdin, not a file path argument. Clean up temp file after.\n3. **Last resort: positional arg** -- only for very short prompts (<4k chars) where the backend accepts inline args\n\nNote: \"temp file fallback\" does NOT mean passing a file path as a CLI argument (most CLIs don't support that). It means using a temp file as an intermediary to feed stdin reliably.\n\nThe transport mode is cached per-backend in `_cli_caps[backend].prompt_mode` (stdin | file_stdin | arg).\n\nExample (claude, stdin mode):\n```python\nproc = subprocess.run(\n    [\"claude\", \"-p\", \"--model\", model,\n     \"--output-format\", \"json\", \"--tools\", \"\", \"--no-session-persistence\",\n     \"--disable-slash-commands\", \"--max-turns\", \"1\"],\n    input=combined_prompt, capture_output=True, text=True, timeout=120,\n)\n```\n\n### Cache key updates\n\nEffectiveness and size-impact runners use cache keys to avoid redundant generations. Cache keys currently hash `model` but not the CLI backend. Update `_prompt_hash()` and `_condition_hash()` to incorporate `cli_backend + resolved_model_string` (even if model is None) so caches are backend-stable and cannot be polluted across different CLI tools.\n\n### Retry logic\n\nKeep `retry_with_backoff()` but adapt it to handle subprocess failures (non-zero exit codes, timeouts) instead of SDK exceptions.\n\n### Cost tracking\n\n- `COST_USD` is **CLI-reported** when available (e.g., `claude --output-format json` includes usage metadata), otherwise 0\n- For `codex`/`copilot` and `claude` text-fallback: cost = 0, tokens = 0\n- `track_cost()` in `_common.py`: update to accept CLI-reported cost directly instead of estimating from SDK model IDs. If CLI reports cost, use it. If not, `COST_USD=0.0` and the call-count cap provides safety. Remove any SDK-model-ID-based estimation tables that would produce misleading numbers with CLI model strings like `haiku` or `o4-mini`.\n- Retain `max_cost_per_run` safety cap alongside `max_calls_per_run`\n\n### Schema invariants\n\n**Critical**: The result envelope `summary` shape for all 4 eval types MUST remain unchanged after migration. `compare_baseline.py` depends on specific scalar keys per eval type. Pinned per-type contracts:\n\n- **L3 Activation**: `summary._overall` contains `tpr`, `fpr`, `accuracy`, `n`, `tpr_stats`, `fpr_stats`, `accuracy_stats`, `true_positives`\n- **L4 Confusion**: `summary[group]` contains `accuracy`, `cross_activation_rate`, `n`, `accuracy_stats`, `cross_activation_stats`, `multi_activation_count`, `no_activation_count`, `total_cases`; `summary._negative_controls` contains `pass_rate`, `passed`, `failed`, `n`\n- **L5 Effectiveness**: `summary[skill]` contains `mean`, `stddev`, `n`, `wins_enhanced`, `wins_baseline`, `ties`, `errors`, `total_cases`, `win_rate` (note: keys are `wins_enhanced`/`wins_baseline`, NOT `wins`/`losses`)\n- **L6 Size Impact**: `summary[candidate]` contains `size_tier`, `body_bytes`, `full_bytes`, `full_tokens_estimated`, `summary_bytes`, `summary_tokens_estimated`, `mean`, `stddev`, `n`, `errors` (flat scalars for `compare_baseline.py`) AND `comparisons` dict with per-comparison-type entries (e.g., `full_vs_baseline`) each containing `mean`, `stddev`, `n`, `wins_full`, `wins_baseline`, `ties` (richer block for quality-bar validation). Both must be present.\n\nVerify by running `compare_baseline.py` after migration against a pre-migration result file. If any key is missing or renamed, the migration is incomplete.\n\n### Skill restore strategy\n\nUnchanged from original .7 spec -- git-based restore mechanism remains valid.\n\n## Acceptance\n- [ ] `get_client()` removed from `_common.py`; replaced with `call_model()` CLI wrapper\n- [ ] No remaining `client.messages.create` calls anywhere under `tests/evals/` (verify: `grep -r \"client.messages.create\" tests/evals/` returns nothing)\n- [ ] No remaining `anthropic` imports anywhere under `tests/evals/` (verify: `grep -r \"import anthropic\\|from anthropic\" tests/evals/` returns nothing)\n- [ ] `anthropic` removed from `requirements.txt`\n- [ ] `config.yaml` has `cli` section with per-backend model names; has `cost.max_calls_per_run`; no `ANTHROPIC_API_KEY` references anywhere in `tests/evals/`\n- [ ] `run_suite.sh` has no `ANTHROPIC_API_KEY` check; parses `TOTAL_CALLS=`/`COST_USD=`/`ABORTED=`/`N_CASES=` keys\n- [ ] All 4 runners accept `--cli {claude,codex,copilot}` override flag\n- [ ] `call_model()` performs one-time per-backend capability detection (JSON output + prompt transport: stdin/file/arg) with actionable diagnostics on failure\n- [ ] Prompt transport is uniform across backends (stdin preferred, file fallback, arg last resort) -- not just claude\n- [ ] `call_model()` parses multiple known JSON shapes with fallback to raw stdout + warning on unknown shape\n- [ ] Cache keys in effectiveness/size-impact runners incorporate CLI backend + resolved model string\n- [ ] `call_model()` works with `claude` CLI (primary path -- must work)\n- [ ] `call_model()` works with `codex` CLI (secondary path)\n- [ ] `call_model()` works with `copilot` CLI (secondary path)\n- [ ] Dual abort: both cost-based and call-count-based caps enforced\n- [ ] `track_cost()` uses CLI-reported cost (not SDK model ID estimation); `COST_USD=0.0` when cost unavailable\n- [ ] Runners emit `TOTAL_CALLS=`/`COST_USD=`/`ABORTED=`/`N_CASES=` on stdout\n- [ ] Result envelope `summary` schema unchanged for all 4 eval types (verify with `compare_baseline.py`)\n- [ ] `python3 tests/evals/run_activation.py --dry-run` still works\n- [ ] Large system prompts (L3/L4 routing index, ~10-15k chars) pass through without arg-length failures\n- [ ] `python3 tests/evals/run_activation.py --skill dotnet-xunit` completes one real CLI call successfully (exercises index-sized system prompt)\n- [ ] `python3 tests/evals/run_confusion_matrix.py --group testing --dry-run` works (verifies confusion index assembly)\n- [ ] `./scripts/validate-skills.sh && ./scripts/validate-marketplace.sh` pass\n\n## Done summary\nReplaced the entire Anthropic SDK API layer with CLI-based subprocess invocations (claude/codex/copilot) across all 4 eval runners, judge module, and shared infrastructure. Added per-backend capability detection with actionable diagnostics, dual abort mechanism (cost + call-count caps), machine-parseable runner output contract, non-retryable CLIConfigError for deterministic failures, and accurate call-count tracking through retry failures.\n## Evidence\n- Commits: 562a7b0ccc5a54cab643b8c1d9bb707b6b2882d6, bbbfcbc, adf46c9, 0891e39, 820fb80, 4576405, fc99de1, 9843bdd\n- Tests: python3 tests/evals/run_activation.py --dry-run, python3 tests/evals/run_confusion_matrix.py --group testing --dry-run, python3 tests/evals/run_effectiveness.py --dry-run, python3 tests/evals/run_size_impact.py --dry-run, ./scripts/validate-skills.sh, ./scripts/validate-marketplace.sh, grep -r client.messages.create tests/evals/, grep -r 'import anthropic' tests/evals/, grep -r ANTHROPIC_API_KEY tests/evals/\n- PRs:"
    },
    {
      "data": {
        "assignee": null,
        "claim_note": "",
        "claimed_at": null,
        "created_at": "2026-02-24T18:40:45.866987Z",
        "depends_on": [
          "fn-60-run-eval-suite-against-skills-and-fix.7"
        ],
        "epic": "fn-60-run-eval-suite-against-skills-and-fix",
        "id": "fn-60-run-eval-suite-against-skills-and-fix.8",
        "priority": null,
        "spec_path": ".flow/tasks/fn-60-run-eval-suite-against-skills-and-fix.8.md",
        "status": "todo",
        "title": "Fix CLI probe and transport reliability",
        "updated_at": "2026-02-24T18:41:59.404331Z"
      },
      "id": "fn-60-run-eval-suite-against-skills-and-fix.8",
      "runtime": {
        "assignee": "claire@novotny.org",
        "claim_note": "",
        "claimed_at": "2026-02-24T22:19:28.211681Z",
        "evidence": {
          "changes": [
            {
              "file": "tests/evals/_common.py",
              "summary": "Probe transport reliability hardening, codex flag fixes, and permanent error classification."
            }
          ],
          "commands": [
            {
              "command": "python3 -m py_compile tests/evals/_common.py",
              "status": "passed"
            },
            {
              "command": "python3 tests/evals/run_activation.py --dry-run",
              "status": "passed"
            },
            {
              "command": "python3 tests/evals/run_activation.py --skill dotnet-xunit",
              "details": "Observed real CLI execution across multiple cases before manual stop for runtime control.",
              "status": "partial"
            },
            {
              "command": "./scripts/validate-skills.sh",
              "status": "passed"
            },
            {
              "command": "./scripts/validate-marketplace.sh",
              "status": "passed"
            }
          ],
          "task": "fn-60-run-eval-suite-against-skills-and-fix.8"
        },
        "status": "done",
        "updated_at": "2026-02-24T22:35:45.592294Z"
      },
      "spec": "# fn-60-run-eval-suite-against-skills-and-fix.8 Fix CLI probe and transport reliability\n\n## Description\n\nThe CLI probe and transport mechanism in `_common.py` has 5 bugs that caused 8 out of 12 Ralph run attempts to fail with 100% error rate (73/73 cases):\n\n1. **Probe failure permanently degrades to arg mode** (lines 174, 259-277): When both stdin and file_stdin probes fail (even transiently), `prompt_mode` defaults to `\"arg\"` and gets cached forever. For claude, the code only emits a warning instead of raising an error. Arg mode has a 4k char limit but eval prompts are 15.7k chars = guaranteed 100% failure.\n\n2. **No probe retry** (lines 235-245): Each transport mode is probed exactly once. A single transient failure (rate limit, slow startup, temp auth issue) permanently disables that mode.\n\n3. **Auth/permanent errors not classified** (lines 518-522): When `_execute_cli()` gets a non-zero exit code, it always raises generic `RuntimeError`. `retry_with_backoff` then retries 3 times. For permanent errors (auth failure, bad flags), this wastes time and never succeeds. The error propagates to the runner loop which records it and moves to the next case -- where the same permanent error recurs.\n\n4. **Codex backend uses wrong command entirely** (lines 188-191, 428-431): The code uses `codex --approval-mode full-auto -m MODEL -q -`. The non-interactive mode is `codex exec`, not `codex` with root-level flags. `--approval-mode` and `-q` don't exist. Any codex invocation immediately fails with exit code 2.\n\n5. **Copilot backend misuses `-p -`** (lines 195, 432-434): The code uses `[\"copilot\", \"-p\", \"-\"]` assuming `-` means \"read from stdin\". It doesn't -- copilot treats `-` as the literal prompt text. The actual stdin content is silently ignored. Also missing required subprocess flags (`-s`, `--no-color`, `--allow-all-tools`, `--no-custom-instructions`).\n\n**Size:** M\n**Files:**\n- `tests/evals/_common.py` -- probe logic, transport, error classification, codex flags\n- `tests/evals/config.yaml` -- probe config\n\n## Authoritative CLI reference (from research)\n\n### Claude CLI (`claude -p`)\n- **stdin**: Fully supported. `echo \"prompt\" | claude -p` is the documented usage mode.\n- **Flags used by eval code**: `--model`, `--system-prompt`, `--output-format json`, `--no-session-persistence`, `--disable-slash-commands`, `--tools \"\"` -- all confirmed valid.\n- **JSON output schema**: `{\"type\": \"result\", \"subtype\": \"success\", \"result\": \"...\", \"total_cost_usd\": 0.42, \"usage\": {\"input_tokens\": N, \"output_tokens\": N}}` -- the `_parse_cli_output()` code handles this correctly.\n- **Exit codes**: 0 = success, 1 = error. stderr contains the error message.\n- **Known issue**: Large stdin inputs (>7000 chars) may produce empty output with exit code 0 (GitHub issue #7263). Eval prompts are ~15.7k chars. If this manifests, the workaround is writing the prompt to a temp file and using file_stdin mode. Monitor for this but don't preemptively switch -- stdin works for the majority of cases in our successful runs.\n- **No TTY required**: Works in non-interactive subprocess context.\n\n### Codex CLI (`codex exec`)\n- **BROKEN in current eval code**: `_build_transport_probe_cmd()` at line 188 uses `[\"codex\", \"--approval-mode\", \"full-auto\", \"-m\", MODEL, \"-q\", \"-\"]`. The non-interactive command is `codex exec`, not `codex` with root flags. `--approval-mode` doesn't exist (it's `-a`). `-q` doesn't exist at all. Immediate exit code 2.\n- **Correct non-interactive command**: `codex exec --full-auto --ephemeral -C /tmp -m MODEL -` where `-` means read prompt from stdin. Confirmed by live test: returns \"OK\" with exit code 0.\n- **`--full-auto`**: Convenience alias for `-a on-request --sandbox workspace-write`. Only available on `exec` subcommand.\n- **`--ephemeral`**: Don't persist session files (important for eval subprocess).\n- **`-C /tmp`**: Set working directory (prevents loading project context, like `_SUBPROCESS_CWD`).\n- **`--json`**: Outputs JSONL events. `turn.completed` includes `usage` with `input_tokens`, `cached_input_tokens`, `output_tokens`. No `cost_usd`. Agent message in `item.completed` with `type: \"agent_message\"`.\n- **`-o FILE`**: Write last agent message to file (alternative to parsing stdout).\n- **stdin**: `-` as the prompt argument reads from stdin. This is documented in `codex exec --help`: \"If not provided as an argument (or if `-` is used), instructions are read from stdin\".\n- **Model selection**: `-m MODEL` (works on `exec` subcommand).\n- **Exit codes**: 0 = success, 2 = argument error, 1 = runtime error.\n- **No `-q` flag**: Does not exist. Use `--json` for structured output or `-o FILE` for clean text output.\n- **Impact**: Any test using `cli.default: codex` will fail immediately with exit code 2.\n- **Action**: Rewrite codex commands to use `codex exec` subcommand with correct flags.\n\n### Copilot CLI (`copilot`)\n- **BROKEN in current eval code**: `_build_transport_probe_cmd()` at line 195 and `_build_cli_command()` at line 434 both use `[\"copilot\", \"-p\", \"-\"]`. The `-` is treated as the **literal prompt text** (the string \"-\"), not as a \"read from stdin\" convention. Stdin content is completely ignored.\n- **stdin**: Fully supported since v0.0.414+. Copilot auto-detects stdin pipe when no `-p` flag is present. Confirmed by live test: `echo \"Reply with exactly: OK\" | copilot -s --no-color --allow-all-tools --no-custom-instructions` returns \"OK\" with exit code 0.\n- **Binary**: Standalone `/opt/homebrew/bin/copilot` (via `brew install copilot-cli`). Also available as `gh copilot` wrapper. Current version: 0.0.415.\n- **Required flags for subprocess use**: `-s` (silent, clean stdout only), `--no-color` (no ANSI), `--allow-all-tools` (non-interactive tool approval), `--no-custom-instructions` (skip AGENTS.md loading).\n- **Model selection**: `--model <choice>` with enumerated values (claude-sonnet-4.6, gpt-5.3-codex, etc.). Config has `copilot.model: null` which uses default.\n- **No JSON output mode**: Correctly detected as unavailable by current code. Output is plain text via stdout (with `-s`).\n- **No `--system-prompt` flag**: Correctly handled by combining system+user into single payload for non-claude backends (line 383).\n- **Exit codes**: 0 = success, 1 = error. Error messages in stderr.\n- **Cost tracking**: Reports \"Premium requests\" not USD. `cost` field will be 0.0 for copilot.\n- **Action**: Fix copilot command in both `_build_transport_probe_cmd()` and `_build_cli_command()`. Remove `-p -`, add `-s --no-color --allow-all-tools --no-custom-instructions`.\n\n## Approach\n\n### Fix 1: Remove transport probing for claude, hardcode stdin\n\nThe claude CLI supports stdin piping. This is the documented usage mode for `claude -p`. The probe mechanism was overengineered -- it tested \"Reply with exactly: OK\" and required exit code 0 + \"OK\" in stdout, which can fail for any transient reason.\n\n**Replace the probe cascade for claude with a direct stdin default.** In `_detect_cli_caps()`:\n- If `backend == \"claude\"`: set `prompt_mode = \"stdin\"` directly. Skip transport probing entirely.\n- Keep probing for `codex` and `copilot` (less certain about their stdin support).\n- Still probe JSON output support for claude (the `--output-format json` probe is independent and useful). Use the hardcoded stdin mode for this probe rather than the probed transport mode -- this avoids the current chicken-and-egg problem where a transport probe failure prevents the JSON probe from running (line 285: `if backend == \"claude\" and prompt_mode != \"arg\"`).\n\n### Fix 2: Kill arg mode fallback for claude\n\nAfter fixing the probe, arg mode should never be reached for claude. But as a safety net: if `prompt_mode` is somehow still `\"arg\"` for claude, raise `CLIConfigError` immediately (matching what codex/copilot already do at line 261-272) instead of silently degrading with a warning.\n\nCurrently (line 259-277):\n- codex/copilot: raise `CLIConfigError` -- correct\n- claude: emit warning, continue with arg mode -- wrong\n\nChange claude to also raise `CLIConfigError`. Arg mode with a 4k limit is useless for eval prompts and only delays the inevitable failure.\n\n### Fix 3: Add error classification to `_execute_cli()`\n\nBefore raising `RuntimeError` at line 521, classify the error by parsing `proc.stderr` and `proc.returncode`:\n\nAdd `CLIPermanentError(CLIConfigError)` -- a new exception subclass for permanent runtime errors (auth, bad flags). Since it extends `CLIConfigError`, `retry_with_backoff` automatically skips retries (line 688-691). Zero changes needed to `retry_with_backoff`.\n\nClassification logic in `_execute_cli()` at the non-zero-exit-code path:\n- Exit code 126 (permission denied) or 127 (command not found): raise `CLIPermanentError`\n- Exit code 2 (argument parsing error, e.g. codex deprecated flags): raise `CLIPermanentError`\n- Stderr matches permanent patterns (`authentication_error`, `Could not resolve authentication`, `401`, `permission_error`, `403`, `unknown option`, `invalid flag`, `unexpected argument`): raise `CLIPermanentError`\n- Stderr matches retryable patterns (`overloaded_error`, `529`, `rate_limit`, `429`, `timeout`): raise `RuntimeError` (retries normally)\n- Unknown: raise `RuntimeError` (default to retryable, conservative)\n\n### Fix 4: Rewrite codex backend to use `codex exec`\n\nIn both `_build_transport_probe_cmd()` (line 188-191) and `_build_cli_command()` (line 428-431):\n- Command base: `[\"codex\", \"exec\", \"--full-auto\", \"--ephemeral\", \"-C\", \"/tmp\"]`\n- Model: `-m MODEL` (same flag, works on `exec` subcommand)\n- stdin: append `-` as the prompt argument (codex exec reads stdin when `-` is the prompt)\n- Remove: `--approval-mode full-auto` (doesn't exist), `-q` (doesn't exist), bare `-` at end of root command\n\n### Fix 5: Fix copilot backend flags\n\nIn both `_build_transport_probe_cmd()` (line 195) and `_build_cli_command()` (line 432-434):\n- Remove `-p` and `-` entirely \u2014 copilot auto-detects stdin pipe\n- Add `-s` (silent mode, clean stdout only)\n- Add `--no-color` (prevent ANSI escape codes)\n- Add `--allow-all-tools` (required for non-interactive execution)\n- Add `--no-custom-instructions` (prevent loading AGENTS.md from cwd, matching claude's `--disable-slash-commands`)\n- Model flag: `--model MODEL` (same syntax, already correct in config handling)\n\n### Fix 6: Reduce probe timeout for codex/copilot\n\nChange probe timeout from 120s to 15s in `_run_probe()` (lines 208, 226). Probes should fail fast. Keep production call timeout at 120s.\n\n## Key context\n\n- `CLIConfigError` at line 33-41 is already non-retryable in `retry_with_backoff` (line 688-691). Making `CLIPermanentError` a subclass means zero changes needed to `retry_with_backoff`.\n- `_cli_caps` cache (line 116) persists for process lifetime. That's fine when the cached value is correct (stdin works). It was a problem when it cached a *wrong* value (arg mode from a failed probe).\n- The probe at line 205 sends \"Reply with exactly: OK\" and checks for \"OK\" in stdout. This is brittle -- the claude CLI might wrap the response in JSON, add prefixes, etc. Removing the probe for claude eliminates this entire class of fragility.\n- The codex breakage (wrong command entirely -- needs `codex exec`, not `codex` with non-existent flags) hasn't blocked Ralph runs because `config.yaml` defaults to `cli.default: claude`. But it makes the codex backend completely non-functional.\n- The copilot `-p -` bug is similarly non-blocking (default is claude), but makes the copilot backend send a literal \"-\" as the prompt, ignoring the real prompt entirely.\n- Both codex and copilot bugs were introduced when the backends were first written -- they were never tested with actual CLI invocations, only with the claude backend.\n- The large stdin issue (GitHub #7263) was not observed in our 4 successful runs (15.7k chars via stdin, worked fine). Monitor but don't preemptively switch to file_stdin.\n## Acceptance\n- [ ] Claude backend skips transport probing -- stdin is the default, no probe needed\n- [ ] Claude JSON output probe still runs (using stdin mode directly, not dependent on transport probe)\n- [ ] codex/copilot still probe transport (existing behavior preserved)\n- [ ] Claude raises `CLIConfigError` if prompt_mode is somehow `\"arg\"` (no silent arg fallback)\n- [ ] `CLIPermanentError` class exists as subclass of `CLIConfigError`\n- [ ] `_execute_cli()` classifies auth errors, bad flags, exit codes 2/126/127 as `CLIPermanentError`\n- [ ] `_execute_cli()` classifies `unexpected argument` in stderr as `CLIPermanentError`\n- [ ] `CLIPermanentError` is not retried by `retry_with_backoff` (inherits CLIConfigError behavior)\n- [ ] Retryable errors (overloaded, rate limit, timeout) still raise `RuntimeError` and retry normally\n- [ ] Codex rewritten to `codex exec --full-auto --ephemeral -C /tmp -m MODEL -` (not root `codex` command)\n- [ ] Codex updated in both `_build_transport_probe_cmd()` and `_build_cli_command()`\n- [ ] Copilot flags fixed: `-p -` removed, stdin auto-detected by copilot when piped\n- [ ] Copilot has `-s --no-color --allow-all-tools --no-custom-instructions` for subprocess use\n- [ ] Copilot flags updated in both `_build_transport_probe_cmd()` and `_build_cli_command()`\n- [ ] Probe timeout reduced to 15s for codex/copilot probes\n- [ ] Production call timeout unchanged at 120s\n- [ ] `python3 tests/evals/run_activation.py --dry-run` still works\n- [ ] `python3 tests/evals/run_activation.py --skill dotnet-xunit` completes one real CLI call\n- [ ] `./scripts/validate-skills.sh && ./scripts/validate-marketplace.sh` pass\n- [ ] No new external dependencies\n## Done summary\nTBD\n## Evidence\n- Commits:\n- Tests:\n- PRs:"
    },
    {
      "data": {
        "assignee": null,
        "claim_note": "",
        "claimed_at": null,
        "created_at": "2026-02-24T18:40:53.845366Z",
        "depends_on": [
          "fn-60-run-eval-suite-against-skills-and-fix.8"
        ],
        "epic": "fn-60-run-eval-suite-against-skills-and-fix",
        "id": "fn-60-run-eval-suite-against-skills-and-fix.9",
        "priority": null,
        "spec_path": ".flow/tasks/fn-60-run-eval-suite-against-skills-and-fix.9.md",
        "status": "todo",
        "title": "Add error classification and fail-fast to eval runners",
        "updated_at": "2026-02-24T18:42:33.173610Z"
      },
      "id": "fn-60-run-eval-suite-against-skills-and-fix.9",
      "runtime": {
        "assignee": "claire@novotny.org",
        "claim_note": "",
        "claimed_at": "2026-02-25T00:31:45.195751Z",
        "evidence": {
          "commits": [
            "4807a4d",
            "9b9166c",
            "bf717bf",
            "54170cf"
          ],
          "prs": [],
          "tests": [
            "python3 -c 'import py_compile; ...' (all 5 Python files)",
            "python3 tests/evals/run_activation.py --dry-run",
            "python3 tests/evals/run_confusion_matrix.py --dry-run",
            "python3 tests/evals/run_effectiveness.py --dry-run",
            "python3 tests/evals/run_size_impact.py --dry-run",
            "bash tests/evals/run_suite.sh --dry-run",
            "ConsecutiveFailureTracker unit tests",
            "retry_with_backoff calls_consumed tests",
            "./scripts/validate-skills.sh",
            "./scripts/validate-marketplace.sh"
          ]
        },
        "status": "done",
        "updated_at": "2026-02-25T01:18:01.276183Z"
      },
      "spec": "# fn-60-run-eval-suite-against-skills-and-fix.9 Add error classification and fail-fast to eval runners\n\n## Description\nWire error classification from task .8 into all 4 eval runners and `run_suite.sh` so that permanent errors abort early instead of running all 73-300+ cases against a broken wall.\n\n**Size:** M\n**Files:**\n- `tests/evals/run_activation.py` -- add consecutive-failure tracking to main loop\n- `tests/evals/run_confusion_matrix.py` -- same\n- `tests/evals/run_effectiveness.py` -- same\n- `tests/evals/run_size_impact.py` -- same\n- `tests/evals/run_suite.sh` -- parse FAIL_FAST, handle skipped runners\n- `tests/evals/config.yaml` -- fail-fast threshold config\n\n## Approach\n\n### ConsecutiveFailureTracker\n\nAdd to `_common.py` (task .8 already modifies this file; this class can be added in .8 or .9 -- implementer's choice, just don't duplicate work):\n\nA simple class tracking consecutive case-level failures with the same error fingerprint:\n- `__init__(threshold: int = 3)` from config\n- `record_failure(exc) -> bool` returns True when threshold breached\n- `record_success()` resets counter\n- `reset()` for multi-run boundaries\n- Fingerprint: first 200 chars of `str(exc)`, whitespace-collapsed\n\n### Runner integration\n\nAll 4 runners follow the same pattern. In each runner's main case loop:\n\nAfter the existing `except Exception as exc:` (which records `api_error`):\n```\nif tracker.record_failure(exc):\n    fail_fast = True\n    fail_fast_reason = tracker.last_fingerprint\n    break\n```\n\nAfter a successful case: `tracker.record_success()`\n\nAt the start of each `--runs` iteration: `tracker.reset()`\n\nMulti-call-per-case runners (effectiveness: 2 gen + 1 judge; size_impact: 3-4 gen + N judge): any unrecoverable exception propagating from the case-level try/except counts as one failure. The tracker operates at the case level, not individual CLI call level.\n\n### Output contract\n\nAdd to each runner's stdout output:\n```\nFAIL_FAST=0|1\nFAIL_FAST_REASON=<fingerprint when 1>\n```\nWhen `FAIL_FAST=1`, also set `ABORTED=1`.\n\nAdd `fail_fast_reason` to results JSON `meta` when applicable.\n\n### Suite orchestrator\n\nIn `run_suite.sh`:\n1. Initialize all cost/call/case variables to `\"0\"` before running (prevents `float(\"\")` crash if a runner is skipped)\n2. Parse `FAIL_FAST` key from each runner's stdout\n3. If `FAIL_FAST=1` with a permanent error reason (auth, permission, CLI not found): skip remaining runners\n4. If `FAIL_FAST=1` with a transient reason: continue to next runner\n5. Add `fail_fast_runners` list to `suite_summary.json`\n\n### Config\n\nAdd to `config.yaml`:\n```yaml\nfail_fast:\n  consecutive_threshold: 3\n  enabled: true\n```\n\nLoad with defaults: `cfg.get(\"fail_fast\", {}).get(\"consecutive_threshold\", 3)`.\n\n## Key context\n\n- All 4 runners already have `except Exception as exc: api_error = str(exc)` at the case level. The tracker wraps around this existing pattern.\n- `run_suite.sh:173-203` sums costs with `float(os.environ.get(..., '0'))`. An empty string from a skipped runner would crash -- must initialize all variables.\n- The existing `ABORTED` key only reflects budget exhaustion. `FAIL_FAST` is a new signal for error-based abort. They're independent but `FAIL_FAST=1` implies `ABORTED=1`.\n- `judge_prompt.invoke_judge()` has its own retry loop for parse failures. Those are expected behavior, not CLI errors. The tracker only sees exceptions that escape the case-level try/except.\n- Task .8 adds `CLIPermanentError(CLIConfigError)` -- this exception type is the primary signal for permanent vs transient classification. `isinstance(exc, CLIConfigError)` catches both `CLIConfigError` and `CLIPermanentError`.\n- **Permanent error patterns** (from .8 research): auth failures, bad flags, exit code 2/126/127, `unexpected argument`. These will never resolve by retrying.\n- **Transient error patterns**: rate limits (429), overloaded (529), timeouts. These may resolve for the next runner or after a delay.\n- The codex backend flags were broken (deprecated `--approval-mode`) and the copilot backend misused `-p -` (literal \"-\" as prompt, ignoring stdin). Task .8 fixes both. If the suite encounters a `CLIPermanentError` from codex or copilot, that's a permanent error for that specific runner but doesn't affect the claude runner -- suite should continue to remaining runners.\n## Acceptance\n- [ ] `ConsecutiveFailureTracker` class exists in `_common.py` (or wherever .8 placed it)\n- [ ] All 4 runners create a tracker and wire it into the main case loop\n- [ ] 3 consecutive same-error failures trigger early abort (not 73)\n- [ ] Tracker resets at start of each `--runs` iteration\n- [ ] Successful cases reset the consecutive counter\n- [ ] `FAIL_FAST=0|1` emitted on stdout by all 4 runners\n- [ ] `FAIL_FAST_REASON=<fingerprint>` emitted when FAIL_FAST=1\n- [ ] FAIL_FAST=1 implies ABORTED=1\n- [ ] Results JSON `meta` includes `fail_fast_reason` when applicable\n- [ ] Partial results written on abort (existing behavior preserved)\n- [ ] `run_suite.sh` parses FAIL_FAST from each runner\n- [ ] Suite skips remaining runners on permanent fail-fast\n- [ ] Suite continues on transient fail-fast\n- [ ] All cost/call variables initialized to \"0\" (no float(\"\") crash on skipped runners)\n- [ ] `config.yaml` has `fail_fast` section with backward-compatible defaults\n- [ ] `python3 tests/evals/run_activation.py --dry-run` still works\n- [ ] `./tests/evals/run_suite.sh --dry-run` still works\n- [ ] `./scripts/validate-skills.sh && ./scripts/validate-marketplace.sh` pass\n## Done summary\nAdded ConsecutiveFailureTracker to _common.py and wired fail-fast behavior into all 4 eval runners (activation, confusion_matrix, effectiveness, size_impact) and run_suite.sh. Runners emit FAIL_FAST/FAIL_FAST_REASON/FAIL_FAST_PERMANENT on stdout; suite uses FAIL_FAST_PERMANENT for robust permanent/transient classification with regex fallback. Restructured effectiveness and size_impact loop order so run_idx is outermost for proper tracker.reset() at --runs boundaries.\n## Evidence\n- Commits: 4807a4d, 9b9166c, bf717bf, 54170cf\n- Tests: python3 -c 'import py_compile; ...' (all 5 Python files), python3 tests/evals/run_activation.py --dry-run, python3 tests/evals/run_confusion_matrix.py --dry-run, python3 tests/evals/run_effectiveness.py --dry-run, python3 tests/evals/run_size_impact.py --dry-run, bash tests/evals/run_suite.sh --dry-run, ConsecutiveFailureTracker unit tests, retry_with_backoff calls_consumed tests, ./scripts/validate-skills.sh, ./scripts/validate-marketplace.sh\n- PRs:"
    }
  ]
}

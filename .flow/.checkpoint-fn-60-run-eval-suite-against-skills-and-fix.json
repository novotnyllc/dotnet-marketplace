{
  "created_at": "2026-02-24T03:43:03.091298Z",
  "epic": {
    "data": {
      "branch_name": "fn-60-run-eval-suite-against-skills-and-fix",
      "completion_review_status": "unknown",
      "completion_reviewed_at": null,
      "created_at": "2026-02-24T01:34:21.523619Z",
      "default_impl": null,
      "default_review": null,
      "default_sync": null,
      "depends_on_epics": [],
      "id": "fn-60-run-eval-suite-against-skills-and-fix",
      "next_task": 1,
      "plan_review_status": "unknown",
      "plan_reviewed_at": "2026-02-24T03:36:11.963893Z",
      "spec_path": ".flow/specs/fn-60-run-eval-suite-against-skills-and-fix.md",
      "status": "open",
      "title": "Run Eval Suite Against Skills and Fix to Quality Bar",
      "updated_at": "2026-02-24T03:36:11.964438Z"
    },
    "spec": "# Run Eval Suite Against Skills and Fix to Quality Bar\n\n## Overview\n\nEpic fn-58 built a comprehensive offline evaluation framework with 4 runners (activation, confusion, effectiveness, size impact), 12 rubrics, 73 activation cases, 36 confusion cases, and 11 size impact candidates. But it never actually **ran** the evals or **fixed** anything based on the results.\n\nThis epic closes the loop: run all 4 eval types against the current 131-skill catalog, analyze failures, fix skill descriptions and content until they hit a reasonable quality bar, and save initial baselines for future regression tracking.\n\n## Quality Bar (\"Good Enough\" Thresholds)\n\nThese thresholds account for the inherent difficulty of each eval type and the fact that we're routing across 131 skills with a small (haiku) model. Perfect scores are not expected.\n\n### L3 Activation\n- **TPR >= 75%** -- 3/4 positive cases route to the correct skill\n- **FPR <= 20%** -- negative controls mostly rejected\n- **Accuracy >= 70%** -- overall correctness across positive + negative\n\n### L4 Confusion\n- **Per-group accuracy >= 60%** -- overlapping skills are inherently hard\n- **Cross-activation rate <= 35%** -- some confusion is expected within groups\n- **No \"never-activated\" skills** -- every skill in a group gets predicted at least once (documented exception allowed if the model systematically cannot route to a niche skill within a small case set). Check via `artifacts.findings[]` where `type == \"never_activated\"`.\n- **Negative control pass rate >= 70%**\n\n### L5 Effectiveness\n- **Overall win rate >= 50%** -- enhanced beats baseline at least half the time\n- **Mean improvement > 0** -- net positive across all skills\n- **No individual skill has 0% win rate** (every skill should help at least sometimes)\n\n### L6 Size Impact\n- **full > baseline** in >= 55% of `full_vs_baseline` comparisons (aggregate across all candidates, excluding errors)\n- **No skill where baseline consistently beats full** -- defined as baseline sweep: `wins_baseline == n` for that candidate's `full_vs_baseline` comparisons (with `--runs 3`, this means baseline wins all 3 runs). This indicates the skill's full content is actively harmful.\n\nThese thresholds are deliberately achievable. If initial results are dramatically below them, it signals description/content problems to fix, not unrealistic targets. If results exceed them, great -- the bar may be raised later.\n\n## Scope\n\n**In scope:**\n- Running all 4 eval types (activation, confusion, effectiveness, size impact)\n- Analyzing results to identify worst-performing skills, descriptions, and confusion pairs\n- Fixing skill frontmatter descriptions to improve activation routing\n- Fixing overlapping skill descriptions to reduce cross-activation\n- Improving skill content for skills with poor effectiveness scores\n- Re-running evals to validate fixes\n- Saving initial baseline JSON files for regression tracking\n- Blocking fn-58.4 (CI workflow) on this epic's completion\n\n**Out of scope:**\n- Adding new rubrics beyond the existing 12\n- Adding new activation/confusion test cases\n- Setting up CI (that's fn-58.4, which this blocks)\n\n## Approach\n\n### Prerequisites (task .7)\n\n**CLI-based API layer**: The eval runners currently use the Anthropic Python SDK (`anthropic.Anthropic().messages.create(...)`) for all LLM calls. This requires `ANTHROPIC_API_KEY` to be set. **This is wrong.** The coding CLI clients (`claude`, `codex`, `copilot`) are already authenticated locally and must be used instead.\n\nTask .7 replaces the entire SDK-based API layer with CLI subprocess invocations:\n\n- **`_common.py`**: Replace `get_client()` and all `client.messages.create()` patterns with a `call_model()` function that shells out to the configured CLI tool\n- **CLI tools** (all locally installed and already authenticated):\n  - `claude -p \"prompt\" --system-prompt \"...\" --model haiku --output-format json --tools \"\"`\n  - `codex exec \"prompt\" -m model`\n  - `copilot -p \"prompt\"`\n- **Config**: `config.yaml` gets a `cli` section specifying which tool to use (default: `claude`). Remove all `ANTHROPIC_API_KEY` references.\n- **No SDK dependency**: Remove `anthropic` from `requirements.txt` for the API call path. Keep `pyyaml` for config parsing.\n\nThe `claude` CLI is the primary backend because it supports `--system-prompt`, `--model`, and `--output-format json`. For `codex` and `copilot`, system prompts are prepended to the user message.\n\n**Skill loading**: Eval runners already load skills from `REPO_ROOT/skills/` (the local checkout). No plugin installation needed -- skills are read directly from the repo, not from any installed plugin location.\n\n**Skill restore**: Tasks .3/.4 modify SKILL.md files. Restore mechanism is git-based:\n- Commit clean state before starting fixes\n- Each fix batch gets its own commit\n- `git checkout -- skills/` restores any file to its committed state\n- `git revert <commit>` undoes an entire fix batch if it makes things worse\n\n### Iteration Strategy\n\nThis is an iterative improve-measure loop:\n\n1. **Run** all 4 eval types against current skills\n2. **Analyze** -- identify worst performers, common failure patterns\n3. **Fix routing** (task .3) -- description edits for activation/confusion\n4. **Fix content** (task .4) -- body edits for effectiveness, executed AFTER .3 to isolate attribution\n5. **Re-run** -- verify improvement, check for regressions\n6. **Save baselines** once thresholds are met\n\n**Important**: Tasks .3 and .4 both edit `skills/*/SKILL.md` but target different sections (frontmatter descriptions vs body content). They should be executed sequentially -- .3 first (routing fixes), then .4 (content fixes) -- even though the dependency graph allows parallelism. This isolates attribution (\"did the improvement come from description or content?\") and avoids merge conflicts on the same files.\n\nExpect 1-3 fix-rerun iterations. Focus on high-leverage fixes first (description clarity for activation/confusion, content quality for effectiveness).\n\n### Cost Expectations\n\nCLI invocations use the CLI's own billing/credits. Token-level cost tracking depends on CLI output format:\n- `claude --output-format json` includes usage metadata -- cost can be extracted\n- `codex`/`copilot` report usage in their own formats -- extraction is best-effort\n\nPer eval runner invocation (approximate):\n- Activation: ~73 CLI calls\n- Confusion: ~54 CLI calls\n- Effectiveness with `--runs 3`: ~216 CLI calls (12 skills x 2 prompts x 3 runs x 3 calls/case)\n- Size impact with `--runs 3`: ~297 CLI calls (11 candidates x ~3 comparison types x 3 runs x 3 calls/case)\n\n`config.yaml:cost.max_cost_per_run` is retained as a safety cap. If cost tracking is unavailable from a given CLI, the cap is based on call count instead.\n\n### Fix Priority\n\n1. **Activation failures** -- fix descriptions so the model can find skills\n2. **Confusion cross-activations** -- differentiate overlapping skills\n3. **Effectiveness losses** -- improve content for skills that underperform baseline\n4. **Size impact anomalies** -- investigate skills where baseline beats full\n\n## Quick Commands\n\n```bash\n# Run each eval type (CLI clients must be authenticated -- no API key needed)\npython3 tests/evals/run_activation.py\npython3 tests/evals/run_confusion_matrix.py\npython3 tests/evals/run_effectiveness.py --runs 3\npython3 tests/evals/run_size_impact.py --runs 3\n\n# Run a single skill/group\npython3 tests/evals/run_activation.py --skill dotnet-xunit\npython3 tests/evals/run_confusion_matrix.py --group testing\npython3 tests/evals/run_effectiveness.py --skill dotnet-xunit --runs 3\n\n# Dry-run (no CLI calls)\npython3 tests/evals/run_activation.py --dry-run\n\n# Override CLI tool\npython3 tests/evals/run_activation.py --cli codex\n```\n\n## Acceptance\n\n- [ ] All 4 eval types have been run at least once with real CLI calls (not dry-run)\n- [ ] Results analyzed and findings documented per task\n- [ ] Skill descriptions fixed where activation/confusion results indicated problems\n- [ ] Skill content improved where effectiveness results showed regression vs baseline\n- [ ] Re-run results meet the quality bar thresholds above (or documented rationale for exceptions)\n- [ ] Initial baseline JSON files saved to `tests/evals/baselines/`\n- [ ] `./scripts/validate-skills.sh && ./scripts/validate-marketplace.sh` still pass\n- [ ] fn-58.4 unblocked (has dependency on this epic's final task)\n\n## Risks\n\n| Risk | Mitigation |\n|------|------------|\n| CLI tool not in PATH | Task .7 validates `claude`/`codex`/`copilot` availability at startup |\n| CLI output format changes | Parse defensively; extract text from structured output with fallbacks |\n| CLI subprocess overhead | Each call spawns a process -- slower than SDK but acceptable for eval workload |\n| Skill modifications need restore | Git-based: commit before fixes, `git checkout -- skills/` to restore |\n| Fixing one skill breaks another | Re-run full suite after each fix batch; baselines track regressions |\n| Quality bar too high for haiku | Thresholds designed for haiku; can lower if systematically unachievable with documented rationale |\n| Description changes break copilot smoke tests | Run `validate-skills.sh` after each change batch |\n| .3/.4 file conflicts | Execute sequentially (.3 first, then .4) even though deps allow parallelism |\n| Cost tracking unavailable | Some CLIs don't expose token counts; fall back to call-count-based caps |\n"
  },
  "epic_id": "fn-60-run-eval-suite-against-skills-and-fix",
  "schema_version": 2,
  "tasks": [
    {
      "data": {
        "created_at": "2026-02-24T01:34:34.323597Z",
        "depends_on": [
          "fn-60-run-eval-suite-against-skills-and-fix.7"
        ],
        "epic": "fn-60-run-eval-suite-against-skills-and-fix",
        "id": "fn-60-run-eval-suite-against-skills-and-fix.1",
        "priority": null,
        "spec_path": ".flow/tasks/fn-60-run-eval-suite-against-skills-and-fix.1.md",
        "status": "todo",
        "title": "Run initial eval suite and capture raw results",
        "updated_at": "2026-02-24T03:23:21.229138Z"
      },
      "id": "fn-60-run-eval-suite-against-skills-and-fix.1",
      "runtime": {
        "status": "todo",
        "updated_at": "2026-02-24T03:23:21.227524Z"
      },
      "spec": "# fn-60.1 Run initial eval suite and capture raw results\n\n## Description\n\nRun all 4 eval types against the current 131-skill catalog to establish the \"before\" state. This is the diagnostic baseline -- no fixes yet, just measurement.\n\n**Size:** M\n**Files:**\n- `tests/evals/results/` (output directory, gitignored)\n\n## Approach\n\nRun each eval type in sequence using CLI-based invocations (task .7 replaces SDK calls with subprocess calls to `claude`/`codex`/`copilot`). CLI clients are already authenticated locally -- no API key setup needed.\n\n1. **L3 Activation**: `python3 tests/evals/run_activation.py` -- 73 cases, ~55 positive + 18 negative\n2. **L4 Confusion**: `python3 tests/evals/run_confusion_matrix.py` -- 36 confusion + 18 negative controls\n3. **L5 Effectiveness**: `python3 tests/evals/run_effectiveness.py --runs 3` -- 12 skills x 2 prompts x 3 runs = 72 cases; each case = 2 generations + 1 judgment = ~216 CLI calls\n4. **L6 Size Impact**: `python3 tests/evals/run_size_impact.py --runs 3` -- 11 candidates, ~99 comparisons with runs=3\n\nOr use the suite runner: `./tests/evals/run_suite.sh`\n\n**Important**: All runners are \"informational, always exit 0\" -- they can also abort on cost/call cap while still writing a partial results JSON. Exit 0 + file existence does NOT guarantee full completion. Always verify coverage completeness (see acceptance).\n\nSave all result JSON files. Record key metrics from each run's summary output for the analysis task.\n\n**Dry-run is NOT acceptable**: This task requires real CLI calls producing real results. Dry-run mode is only for verifying dataset/config, not for task completion.\n\n## Acceptance\n- [ ] All 4 eval runners complete with real CLI calls (not dry-run) without errors (exit 0) AND without cost-cap abort\n- [ ] Coverage completeness verified for each runner:\n  - Activation: `summary._overall.n` matches expected dataset case count; no \"ABORT\" in output\n  - Confusion: per-group `summary[group].n` matches expected group case count; `_negative_controls.n` matches expected negative case count\n  - Effectiveness: per-skill `summary[skill].total_cases == len(rubric.test_prompts) * runs`; `errors == 0` (or explicitly documented)\n  - Size impact: per-skill comparison counts match expected `runs` for each comparison type\n- [ ] Result JSON files exist in `tests/evals/results/` for all 4 eval types\n- [ ] Key metrics captured:\n  - L3: activation TPR/FPR/accuracy (from `summary._overall`)\n  - L4: confusion per-group accuracy, negative control pass rate, never-activated count (from `artifacts.findings` where `type == \"never_activated\"`)\n  - L5: effectiveness per-skill win rates, confirmation no skill has 0% win rate\n  - L6: size impact `full_vs_baseline` results per candidate\n- [ ] Total cost documented (if available from CLI output; otherwise call count documented)\n- [ ] `./scripts/validate-skills.sh && ./scripts/validate-marketplace.sh` still pass (no changes to skills yet)\n\n## Done summary\nTBD\n\n## Evidence\n- Commits:\n- Tests:\n- PRs:\n"
    },
    {
      "data": {
        "created_at": "2026-02-24T01:34:37.663196Z",
        "depends_on": [
          "fn-60-run-eval-suite-against-skills-and-fix.1"
        ],
        "epic": "fn-60-run-eval-suite-against-skills-and-fix",
        "id": "fn-60-run-eval-suite-against-skills-and-fix.2",
        "priority": null,
        "spec_path": ".flow/tasks/fn-60-run-eval-suite-against-skills-and-fix.2.md",
        "status": "todo",
        "title": "Analyze results and triage findings",
        "updated_at": "2026-02-24T03:23:46.599129Z"
      },
      "id": "fn-60-run-eval-suite-against-skills-and-fix.2",
      "runtime": {
        "status": "todo",
        "updated_at": "2026-02-24T03:23:46.597971Z"
      },
      "spec": "# fn-60.2 Analyze results and triage findings\n\n## Description\n\nAnalyze the raw results from task .1 across all 4 eval types. Identify worst performers, common failure patterns, and produce a prioritized fix list. This analysis drives tasks .3 and .4.\n\n**Size:** M\n**Files:**\n- No file changes -- analysis only, findings documented in this task's summary\n\n## Approach\n\nFor each eval type, extract and document:\n\n### L3 Activation\n- Overall TPR, FPR, accuracy vs thresholds (TPR>=75%, FPR<=20%, accuracy>=70%) -- read from `summary._overall`\n- Per-skill activation rates -- which skills are never found? Read from `artifacts.per_skill`\n- Detection method breakdown (structured vs fallback vs parse_failure) -- read from `artifacts.detection_method_counts`\n- False positives -- which negative controls failed?\n\n### L4 Confusion\n- Per-group accuracy vs threshold (>=60%) -- read from group-level summary\n- Cross-activation rate per group vs threshold (<=35%)\n- Flagged cross-activation pairs (>20% rate)\n- Never-activated skills -- check `artifacts.findings[]` where `type == \"never_activated\"` (must be empty to meet quality bar, or document exception)\n- Cross-check: `artifacts.confusion_matrices.<group>.matrix` column-sum == 0 indicates a never-activated skill\n- Negative control pass rate vs threshold (>=70%)\n\n### L5 Effectiveness\n- Per-skill win rates vs threshold\n- **Overall win rate**: sum(wins) / sum(non-error cases) across all skills (weighted by case count, excluding errors)\n- **Mean improvement**: weighted mean by `n` across skills\n- Confirm no skill has 0% win rate (quality bar requirement)\n- Error rates -- any skills with generation/judge failures?\n- Per-criterion breakdown -- which criteria drive wins/losses?\n\n### L6 Size Impact\n- Compute from `results.summary[skill].comparisons.full_vs_baseline` only (not full_vs_summary or other comparison types)\n- **full > baseline rate**: aggregate `wins_full / (wins_full + wins_baseline + ties)` across all candidate skills (excluding error cases) vs threshold (>=55%)\n- **Per-skill harmful check**: flag any candidate where baseline sweeps all runs (`wins_baseline == n`) in `full_vs_baseline` -- this indicates the full content is actively harmful for that skill\n- full vs summary comparison -- informational only, not part of quality bar\n- Size tier correlations -- do large skills benefit more from full content?\n\n### Triage Output\n- Prioritized list of skills to fix (worst first)\n- Classification: description problem (L3/L4) vs content problem (L5)\n- Estimated fix complexity per skill\n\n## Acceptance\n- [ ] All 4 eval type results analyzed using correct output paths (summary vs artifacts)\n- [ ] Current metrics vs quality bar thresholds documented (including never-activated and negative-control checks)\n- [ ] L6 metrics computed exclusively from `full_vs_baseline` comparisons; \"harmful\" = baseline sweep (wins_baseline == n)\n- [ ] Aggregation method for L5/L6 \"overall\" metrics explicitly stated (weighted by case count)\n- [ ] Prioritized fix list with classification (description vs content)\n- [ ] Fix list informs task .3 (routing fixes) and task .4 (content fixes)\n- [ ] Any threshold adjustments documented with rationale\n\n## Done summary\nTBD\n\n## Evidence\n- Commits:\n- Tests:\n- PRs:\n"
    },
    {
      "data": {
        "assignee": null,
        "claim_note": "",
        "claimed_at": null,
        "created_at": "2026-02-24T01:34:41.008238Z",
        "depends_on": [
          "fn-60-run-eval-suite-against-skills-and-fix.2"
        ],
        "epic": "fn-60-run-eval-suite-against-skills-and-fix",
        "id": "fn-60-run-eval-suite-against-skills-and-fix.3",
        "priority": null,
        "spec_path": ".flow/tasks/fn-60-run-eval-suite-against-skills-and-fix.3.md",
        "status": "todo",
        "title": "Fix activation and confusion routing issues",
        "updated_at": "2026-02-24T01:56:54.621113Z"
      },
      "id": "fn-60-run-eval-suite-against-skills-and-fix.3",
      "runtime": null,
      "spec": "# fn-60.3 Fix activation and confusion routing issues\n\n## Description\n\nBased on the analysis from task .2, fix skill descriptions (frontmatter) to improve activation routing accuracy and reduce confusion between overlapping skills.\n\n**Size:** L\n**Files:**\n- `skills/*/SKILL.md` -- frontmatter description updates for skills identified in .2\n\n**Execution order**: Run this task BEFORE task .4 (even though deps allow parallelism) to isolate attribution of improvements and avoid file conflicts.\n\n## Approach\n\n### Activation Fixes\n- Skills that were never activated in L3: make descriptions more specific and action-oriented\n- Skills with low activation rates: add key differentiating terms to descriptions\n- Follow the routing style guide: Action + Domain + Differentiator formula (docs/skill-routing-style-guide.md)\n- Keep descriptions under 120 characters\n\n### Confusion Fixes\n- For flagged cross-activation pairs (>20% rate): differentiate the two skills' descriptions\n- For low-discrimination skills: add unique qualifying terms\n- Focus on the 7 domain groups: testing, security, data, performance, api, cicd, blazor\n- Ensure each skill in a group has a clearly distinct \"action\" or \"domain\" keyword\n\n### Validation\n- Run `./scripts/validate-skills.sh` after each batch of changes\n- Quick-verify with `python3 tests/evals/run_activation.py --dry-run` (index char count should stay reasonable)\n- Spot-check individual skills with `python3 tests/evals/run_activation.py --skill <name>`\n\n## Acceptance\n- [ ] All skills identified in .2 analysis have updated descriptions\n- [ ] Descriptions still under 120 characters each\n- [ ] Total description budget is budget-neutral or reduced (no increase from pre-fix baseline; target staying under 12,000 chars)\n- [ ] `./scripts/validate-skills.sh && ./scripts/validate-marketplace.sh` pass\n- [ ] Changes follow routing style guide conventions\n"
    },
    {
      "data": {
        "assignee": null,
        "claim_note": "",
        "claimed_at": null,
        "created_at": "2026-02-24T01:34:44.467374Z",
        "depends_on": [
          "fn-60-run-eval-suite-against-skills-and-fix.2"
        ],
        "epic": "fn-60-run-eval-suite-against-skills-and-fix",
        "id": "fn-60-run-eval-suite-against-skills-and-fix.4",
        "priority": null,
        "spec_path": ".flow/tasks/fn-60-run-eval-suite-against-skills-and-fix.4.md",
        "status": "todo",
        "title": "Fix effectiveness issues for low-scoring skills",
        "updated_at": "2026-02-24T01:36:36.863216Z"
      },
      "id": "fn-60-run-eval-suite-against-skills-and-fix.4",
      "runtime": null,
      "spec": "# fn-60.4 Fix effectiveness issues for low-scoring skills\n\n## Description\n\nBased on the analysis from task .2, improve skill content for skills where the enhanced (with-skill) output loses to or ties with the baseline (without-skill) output in L5 effectiveness evals.\n\n**Size:** M\n**Files:**\n- `skills/*/SKILL.md` -- content improvements for underperforming skills (body, not frontmatter)\n\n## Approach\n\n### Content Fixes\n- For skills with 0% win rate: investigate what the judge criteria expect vs what the skill provides\n- For skills with <50% win rate: identify which criteria the skill loses on (per_criterion_breakdown)\n- Common patterns to check:\n  - Skill content is too generic (not specific enough for the eval prompts)\n  - Skill content is outdated (references old APIs or patterns)\n  - Skill content lacks concrete examples that would help generation\n  - Scope section doesn't cover the test prompt topic\n\n### Fix Strategy\n- Prioritize skills where wins_enhanced=0 (total failures)\n- For partial failures: check which rubric criteria drive the losses\n- Add or improve examples, code patterns, or specific guidance\n- Do NOT rewrite entire skills -- targeted improvements only\n\n### Validation\n- Spot-check individual skills: `python3 tests/evals/run_effectiveness.py --skill <name> --runs 3 --regenerate`\n- `./scripts/validate-skills.sh` after changes\n\n## Acceptance\n- [ ] All skills with 0% win rate from .2 analysis have been investigated and improved\n- [ ] Skills with <50% win rate have targeted content improvements\n- [ ] `./scripts/validate-skills.sh && ./scripts/validate-marketplace.sh` pass\n- [ ] Individual skill re-runs show improvement (documented in done summary)\n\n## Done summary\nTBD\n\n## Evidence\n- Commits:\n- Tests:\n- PRs:\n"
    },
    {
      "data": {
        "assignee": null,
        "claim_note": "",
        "claimed_at": null,
        "created_at": "2026-02-24T01:34:48.403802Z",
        "depends_on": [
          "fn-60-run-eval-suite-against-skills-and-fix.3",
          "fn-60-run-eval-suite-against-skills-and-fix.4"
        ],
        "epic": "fn-60-run-eval-suite-against-skills-and-fix",
        "id": "fn-60-run-eval-suite-against-skills-and-fix.5",
        "priority": null,
        "spec_path": ".flow/tasks/fn-60-run-eval-suite-against-skills-and-fix.5.md",
        "status": "todo",
        "title": "Rerun eval suite and validate against quality bar",
        "updated_at": "2026-02-24T02:13:07.377945Z"
      },
      "id": "fn-60-run-eval-suite-against-skills-and-fix.5",
      "runtime": null,
      "spec": "# fn-60.5 Rerun eval suite and validate against quality bar\n\n## Description\n\nFull rerun of all 4 eval types after fixes from tasks .3 and .4. Validate results against the quality bar thresholds defined in the epic spec. If thresholds are not met, do one more targeted fix-and-rerun iteration.\n\n**Size:** M\n**Files:**\n- `tests/evals/results/` (output directory, gitignored)\n\n## Approach\n\n1. Run all 4 eval types with `--regenerate` flag (where applicable) to ensure fresh generations (CLI clients handle auth -- no API key needed):\n   - `python3 tests/evals/run_activation.py`\n   - `python3 tests/evals/run_confusion_matrix.py`\n   - `python3 tests/evals/run_effectiveness.py --runs 3 --regenerate`\n   - `python3 tests/evals/run_size_impact.py --runs 3 --regenerate`\n\n2. Verify coverage completeness (runners exit 0 but may abort on cost/call cap -- see fn-60.1 acceptance for per-runner completeness checks). Re-run with raised cap if any runner aborted.\n\n3. Compare results against ALL quality bar thresholds:\n   - L3: TPR>=75%, FPR<=20%, accuracy>=70% (from `summary._overall`)\n   - L4: per-group accuracy>=60%, cross-activation<=35%, no never-activated skills (check `artifacts.findings[]` where `type == \"never_activated\"` -- must be empty or exception documented), negative control pass rate>=70%\n   - L5: overall win rate>=50% (sum wins / sum non-error cases across skills), mean improvement>0 (weighted by n), no skill at 0% win rate\n   - L6 (computed exclusively from `full_vs_baseline` comparisons):\n     - Aggregate: `wins_full / (wins_full + wins_baseline + ties)` >= 55% across all candidates\n     - Per-skill: no candidate where baseline sweeps all runs (`wins_baseline == n`) -- this indicates harmful skill content\n\n4. If any threshold is missed:\n   - Identify the specific failures\n   - Make targeted fixes (another iteration of .3/.4 work)\n   - Rerun the failing eval type only\n   - Document the iteration in the summary\n\n5. If a threshold proves systematically unachievable (e.g., haiku consistently can't route to certain skills), document the rationale and adjust the threshold.\n\n## Acceptance\n- [ ] All 4 eval types re-run with fresh results and verified complete (no cost-cap aborts)\n- [ ] Results compared against ALL quality bar thresholds including:\n  - L4 \"no never-activated skills\" via `artifacts.findings[]` (or documented exception with rationale)\n  - L4 negative control pass rate >= 70%\n  - L5 no skill at 0% win rate\n  - L5/L6 aggregation uses weighted method (sum wins / sum non-error cases)\n  - L6 per-skill guard: no candidate where baseline sweeps all runs in `full_vs_baseline`\n- [ ] All thresholds met OR documented rationale for exceptions\n- [ ] Before/after comparison documented (initial run vs final run)\n- [ ] Total cost across all iterations documented\n- [ ] `./scripts/validate-skills.sh && ./scripts/validate-marketplace.sh` still pass\n"
    },
    {
      "data": {
        "assignee": null,
        "claim_note": "",
        "claimed_at": null,
        "created_at": "2026-02-24T01:34:51.977858Z",
        "depends_on": [
          "fn-60-run-eval-suite-against-skills-and-fix.5"
        ],
        "epic": "fn-60-run-eval-suite-against-skills-and-fix",
        "id": "fn-60-run-eval-suite-against-skills-and-fix.6",
        "priority": null,
        "spec_path": ".flow/tasks/fn-60-run-eval-suite-against-skills-and-fix.6.md",
        "status": "todo",
        "title": "Save initial baselines and unblock fn-58.4",
        "updated_at": "2026-02-24T02:04:00.205192Z"
      },
      "id": "fn-60-run-eval-suite-against-skills-and-fix.6",
      "runtime": null,
      "spec": "# fn-60.6 Save initial baselines and unblock fn-58.4\n\n## Description\n\nSave the passing eval results as initial baseline JSON files in `tests/evals/baselines/`. These baselines are used by `compare_baseline.py` for future informational regression detection.\n\n**Size:** S\n**Files:**\n- `tests/evals/baselines/effectiveness_baseline.json` (new)\n- `tests/evals/baselines/activation_baseline.json` (new)\n- `tests/evals/baselines/confusion_baseline.json` (new)\n- `tests/evals/baselines/size_impact_baseline.json` (new)\n\n## Approach\n\n1. Extract the `summary` object from each eval type's result JSON\n2. Save with a small `meta` header (run_id, timestamp) as per fn-58.4 spec\n3. Baseline format: `{\"meta\": {...}, \"summary\": {...}}`\n4. `compare_baseline.py` ignores `cases` and `artifacts` fields -- only `summary` matters\n5. Verify baseline files work with: `python3 tests/evals/compare_baseline.py --eval-type <type>`\n6. Replace `.gitkeep` files in `baselines/` directory with real files\n7. Verify fn-58.4 depends on fn-60.6; add dependency if missing\n\n**Note**: `compare_baseline.py` performs informational regression detection (did metrics get worse?), NOT quality-bar validation. Quality bar validation is task .5's responsibility using the raw result JSONs directly. Baselines serve as the \"last known good\" reference for future regression checks only.\n\n## Acceptance\n- [ ] All 4 baseline JSON files exist in `tests/evals/baselines/`\n- [ ] Each baseline contains `meta` and `summary` keys\n- [ ] `compare_baseline.py` runs without error against all 4 baseline types\n- [ ] fn-58.4 dependency on fn-60.6 verified (added if missing)\n- [ ] `./scripts/validate-skills.sh && ./scripts/validate-marketplace.sh` pass\n"
    },
    {
      "data": {
        "created_at": "2026-02-24T01:42:09.523578Z",
        "depends_on": [],
        "epic": "fn-60-run-eval-suite-against-skills-and-fix",
        "id": "fn-60-run-eval-suite-against-skills-and-fix.7",
        "priority": 0,
        "spec_path": ".flow/tasks/fn-60-run-eval-suite-against-skills-and-fix.7.md",
        "status": "todo",
        "title": "Replace SDK API layer with CLI-based invocations",
        "updated_at": "2026-02-24T03:23:21.032352Z"
      },
      "id": "fn-60-run-eval-suite-against-skills-and-fix.7",
      "runtime": {
        "status": "todo",
        "updated_at": "2026-02-24T03:23:21.030765Z"
      },
      "spec": "# fn-60.7 Replace SDK API layer with CLI-based invocations\n\n## Description\n\nThe eval runners currently use the Anthropic Python SDK (`anthropic.Anthropic().messages.create(...)`) for all LLM calls. This requires `ANTHROPIC_API_KEY` and does NOT reflect what actual coding clients do. The CLI clients (`claude`, `codex`, `copilot`) are already authenticated locally -- use them directly via subprocess.\n\nThis task replaces the entire SDK-based API layer with CLI subprocess invocations. All 4 runners and the shared judge module must be updated.\n\n**Size:** L\n**Files:**\n- `tests/evals/_common.py` -- replace `get_client()` with `call_model()` CLI wrapper; remove `anthropic` import\n- `tests/evals/run_activation.py` -- replace `client.messages.create()` calls with `call_model()`\n- `tests/evals/run_confusion_matrix.py` -- replace `client.messages.create()` calls with `call_model()`\n- `tests/evals/run_effectiveness.py` -- replace `client.messages.create()` calls with `call_model()`\n- `tests/evals/run_size_impact.py` -- replace `client.messages.create()` calls with `call_model()`\n- `tests/evals/judge_prompt.py` -- replace `client.messages.create()` in `invoke_judge()` with `call_model()`\n- `tests/evals/config.yaml` -- add `cli` section; remove `ANTHROPIC_API_KEY` references\n- `tests/evals/requirements.txt` -- remove `anthropic` (keep `pyyaml`)\n- `tests/evals/run_suite.sh` -- remove `ANTHROPIC_API_KEY` check\n\n## Approach\n\n### New `call_model()` function in `_common.py`\n\nReplace `get_client()` with a `call_model()` function:\n\n```python\ndef call_model(\n    system_prompt: str,\n    user_prompt: str,\n    model: Optional[str] = None,\n    max_tokens: int = 4096,\n    temperature: float = 0.0,\n    cli: Optional[str] = None,\n) -> dict:\n    \"\"\"Call an LLM via CLI subprocess.\n\n    Returns:\n        {\"text\": str, \"cost\": float, \"input_tokens\": int, \"output_tokens\": int}\n        cost/token fields are 0 if not available from the CLI output.\n    \"\"\"\n```\n\nImplementation per CLI tool:\n\n**`claude` (primary)**:\n```bash\nclaude -p \"USER_PROMPT\" \\\n  --system-prompt \"SYSTEM_PROMPT\" \\\n  --model MODEL \\\n  --output-format json \\\n  --tools \"\" \\\n  --no-session-persistence \\\n  --disable-slash-commands\n```\n- `--tools \"\"` disables tool use (pure text generation)\n- `--output-format json` returns structured JSON with response and usage\n- `--no-session-persistence` prevents session state from accumulating\n- `--disable-slash-commands` prevents skill loading during eval\n- Parse response text and usage from JSON output\n- Pass prompt via stdin (pipe) to avoid shell escaping issues with large prompts\n\n**`codex`**:\n```bash\ncodex exec \"SYSTEM_CONTEXT\\n\\nUSER_PROMPT\" -m MODEL\n```\n- No `--system-prompt` flag -- prepend system prompt to user message\n- Model names differ (e.g., `o3`, `o4-mini`)\n\n**`copilot`**:\n```bash\ncopilot -p \"SYSTEM_CONTEXT\\n\\nUSER_PROMPT\"\n```\n- No `--system-prompt` flag -- prepend system prompt to user message\n- Uses its own model selection\n\n### Config changes\n\n```yaml\n# config.yaml\ncli:\n  default: claude           # which CLI tool to use (claude | codex | copilot)\n  claude:\n    model: haiku            # claude model alias\n  codex:\n    model: o4-mini          # codex model name\n  copilot:\n    model: null             # copilot picks its own model\n\n# Remove these:\n# models:\n#   generation_model: claude-haiku-4-20250514\n#   judge_model: claude-haiku-4-20250514\n# Environment variable overrides: ANTHROPIC_API_KEY...\n```\n\n### Runner changes\n\nEach runner's `main()` function currently does:\n```python\nclient = _common.get_client()\n# ... then later:\nresponse = client.messages.create(model=..., system=..., messages=[...])\ntext = response.content[0].text\n```\n\nReplace with:\n```python\nresult = _common.call_model(system_prompt=..., user_prompt=..., model=...)\ntext = result[\"text\"]\n```\n\nAll 8 `client.messages.create()` call sites across the 4 runners and judge_prompt.py must be updated. Remove the `client` variable entirely.\n\n### run_suite.sh changes\n\nRemove the `ANTHROPIC_API_KEY` check block (lines 47-53). Remove `ANTHROPIC_API_KEY` from the usage comment. The suite just runs -- CLI clients handle their own auth.\n\n### Prompt passing strategy\n\nLarge prompts (routing indices, skill bodies) can exceed shell argument limits. Pass prompts via **stdin pipe** to avoid escaping/length issues:\n```python\nproc = subprocess.run(\n    [\"claude\", \"-p\", \"--system-prompt\", system_prompt, \"--model\", model,\n     \"--output-format\", \"json\", \"--tools\", \"\", \"--no-session-persistence\",\n     \"--disable-slash-commands\"],\n    input=user_prompt, capture_output=True, text=True, timeout=120,\n)\n```\n\n### Retry logic\n\nKeep `retry_with_backoff()` but adapt it to handle subprocess failures (non-zero exit codes, timeouts) instead of SDK exceptions.\n\n### Cost tracking\n\n- For `claude --output-format json`: parse usage metadata from JSON output if available\n- For other CLIs: set cost/token fields to 0\n- Retain `max_cost_per_run` safety cap; if cost data is unavailable, fall back to call-count-based limit (`max_calls_per_run`)\n\n### Skill restore strategy\n\nUnchanged from original .7 spec -- git-based restore mechanism remains valid.\n\n## Acceptance\n- [ ] `get_client()` removed from `_common.py`; replaced with `call_model()` CLI wrapper\n- [ ] All 8 `client.messages.create()` call sites replaced across runners and judge_prompt.py\n- [ ] `anthropic` removed from `requirements.txt`\n- [ ] No remaining imports of `anthropic` in any eval file\n- [ ] `config.yaml` has `cli` section; no `ANTHROPIC_API_KEY` references anywhere in `tests/evals/`\n- [ ] `run_suite.sh` has no `ANTHROPIC_API_KEY` check\n- [ ] `call_model()` works with `claude` CLI (primary path -- must work)\n- [ ] `call_model()` works with `codex` CLI (secondary path)\n- [ ] `call_model()` works with `copilot` CLI (secondary path)\n- [ ] `python3 tests/evals/run_activation.py --dry-run` still works\n- [ ] `python3 tests/evals/run_activation.py --skill dotnet-xunit` completes one real CLI call successfully\n- [ ] `./scripts/validate-skills.sh && ./scripts/validate-marketplace.sh` pass\n\n## Done summary\nTBD\n\n## Evidence\n- Commits:\n- Tests:\n- PRs:\n"
    }
  ]
}

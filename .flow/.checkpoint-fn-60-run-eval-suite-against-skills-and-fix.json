{
  "created_at": "2026-02-24T01:45:44.931917Z",
  "epic": {
    "data": {
      "branch_name": "fn-60-run-eval-suite-against-skills-and-fix",
      "created_at": "2026-02-24T01:34:21.523619Z",
      "depends_on_epics": [],
      "id": "fn-60-run-eval-suite-against-skills-and-fix",
      "next_task": 1,
      "plan_review_status": "unknown",
      "plan_reviewed_at": null,
      "spec_path": ".flow/specs/fn-60-run-eval-suite-against-skills-and-fix.md",
      "status": "open",
      "title": "Run Eval Suite Against Skills and Fix to Quality Bar",
      "updated_at": "2026-02-24T01:43:04.107034Z"
    },
    "spec": "# Run Eval Suite Against Skills and Fix to Quality Bar\n\n## Overview\n\nEpic fn-58 built a comprehensive offline evaluation framework with 4 runners (activation, confusion, effectiveness, size impact), 12 rubrics, 73 activation cases, 36 confusion cases, and 11 size impact candidates. But it never actually **ran** the evals or **fixed** anything based on the results.\n\nThis epic closes the loop: run all 4 eval types against the current 131-skill catalog, analyze failures, fix skill descriptions and content until they hit a reasonable quality bar, and save initial baselines for future regression tracking.\n\n## Quality Bar (\"Good Enough\" Thresholds)\n\nThese thresholds account for the inherent difficulty of each eval type and the fact that we're routing across 131 skills with a small (haiku) model. Perfect scores are not expected.\n\n### L3 Activation\n- **TPR >= 75%** -- 3/4 positive cases route to the correct skill\n- **FPR <= 20%** -- negative controls mostly rejected\n- **Accuracy >= 70%** -- overall correctness across positive + negative\n\n### L4 Confusion\n- **Per-group accuracy >= 60%** -- overlapping skills are inherently hard\n- **Cross-activation rate <= 35%** -- some confusion is expected within groups\n- **No \"never-activated\" skills** -- every skill in a group gets predicted at least once\n- **Negative control pass rate >= 70%**\n\n### L5 Effectiveness\n- **Overall win rate >= 50%** -- enhanced beats baseline at least half the time\n- **Mean improvement > 0** -- net positive across all skills\n- **No individual skill has 0% win rate** (every skill should help at least sometimes)\n\n### L6 Size Impact\n- **full > baseline** in >= 55% of comparisons\n- **No skill where baseline consistently beats full** (that would mean skill content is harmful)\n\nThese thresholds are deliberately achievable. If initial results are dramatically below them, it signals description/content problems to fix, not unrealistic targets. If results exceed them, great -- the bar may be raised later.\n\n## Scope\n\n**In scope:**\n- Running all 4 eval types (activation, confusion, effectiveness, size impact)\n- Analyzing results to identify worst-performing skills, descriptions, and confusion pairs\n- Fixing skill frontmatter descriptions to improve activation routing\n- Fixing overlapping skill descriptions to reduce cross-activation\n- Improving skill content for skills with poor effectiveness scores\n- Re-running evals to validate fixes\n- Saving initial baseline JSON files for regression tracking\n- Blocking fn-58.4 (CI workflow) on this epic's completion\n\n**Out of scope:**\n- Adding new rubrics beyond the existing 12\n- Adding new activation/confusion test cases\n- Setting up CI (that's fn-58.4, which this blocks)\n\n## Approach\n\n### Prerequisites (task .7)\n\n**Auth**: `_common.py:get_client()` currently hard-requires `ANTHROPIC_API_KEY` env var. Fix this to let the Anthropic SDK handle auth discovery -- the local client is already authenticated and doesn't need an explicit key. The SDK checks env vars itself; the pre-validation just gets in the way.\n\n**Skill loading**: Eval runners already load skills from `REPO_ROOT/skills/` (the local checkout). No plugin installation needed -- skills are read directly from the repo, not from any installed plugin location.\n\n**Skill restore**: Tasks .3/.4 modify SKILL.md files. Restore mechanism is git-based:\n- Commit clean state before starting fixes\n- Each fix batch gets its own commit\n- `git checkout -- skills/` restores any file to its committed state\n- `git revert <commit>` undoes an entire fix batch if it makes things worse\n\n### Iteration Strategy\n\nThis is an iterative improve-measure loop:\n\n1. **Run** all 4 eval types against current skills\n2. **Analyze** -- identify worst performers, common failure patterns\n3. **Fix** -- targeted description edits, content improvements\n4. **Re-run** -- verify improvement, check for regressions\n5. **Save baselines** once thresholds are met\n\nExpect 1-3 fix-rerun iterations. Focus on high-leverage fixes first (description clarity for activation/confusion, content quality for effectiveness).\n\n### Cost Expectations\n\nUsing haiku for generation and judging (per config.yaml):\n- Activation: ~73 API calls = ~$0.10-0.20\n- Confusion: ~54 API calls = ~$0.10-0.15\n- Effectiveness: ~24 generations + 24 baselines + 24 judgments = ~$1-2\n- Size impact: ~36 generations + 36 judgments = ~$1-2\n- Total per full run: ~$2-4\n- Budget for 3 iterations: ~$12\n\n### Fix Priority\n\n1. **Activation failures** -- fix descriptions so the model can find skills\n2. **Confusion cross-activations** -- differentiate overlapping skills\n3. **Effectiveness losses** -- improve content for skills that underperform baseline\n4. **Size impact anomalies** -- investigate skills where baseline beats full\n\n## Quick Commands\n\n```bash\n# Run each eval type\npython3 tests/evals/run_activation.py\npython3 tests/evals/run_confusion_matrix.py\npython3 tests/evals/run_effectiveness.py --runs 3\npython3 tests/evals/run_size_impact.py\n\n# Run a single skill/group\npython3 tests/evals/run_activation.py --skill dotnet-xunit\npython3 tests/evals/run_confusion_matrix.py --group testing\npython3 tests/evals/run_effectiveness.py --skill dotnet-xunit --runs 3\n\n# Dry-run (no API calls)\npython3 tests/evals/run_activation.py --dry-run\n```\n\n## Acceptance\n\n- [ ] All 4 eval types have been run at least once with real API calls\n- [ ] Results analyzed and findings documented per task\n- [ ] Skill descriptions fixed where activation/confusion results indicated problems\n- [ ] Skill content improved where effectiveness results showed regression vs baseline\n- [ ] Re-run results meet the quality bar thresholds above (or document rationale for exceptions)\n- [ ] Initial baseline JSON files saved to `tests/evals/baselines/`\n- [ ] `./scripts/validate-skills.sh && ./scripts/validate-marketplace.sh` still pass\n- [ ] fn-58.4 unblocked (has dependency on this epic's final task)\n\n## Risks\n\n| Risk | Mitigation |\n|------|------------|\n| Auth not discoverable | Task .7 fixes `_common.py` to use SDK auth discovery; no explicit key needed |\n| Skill modifications need restore | Git-based: commit before fixes, `git checkout -- skills/` to restore |\n| Cost overrun from multiple iterations | Haiku pricing is cheap (~$3/run); generation caching avoids re-generation |\n| Fixing one skill breaks another | Re-run full suite after each fix batch; baselines track regressions |\n| Quality bar too high for haiku | Thresholds designed for haiku; can lower if systematically unachievable |\n| Description changes break copilot smoke tests | Run `validate-skills.sh` after each change batch |\n"
  },
  "epic_id": "fn-60-run-eval-suite-against-skills-and-fix",
  "schema_version": 2,
  "tasks": [
    {
      "data": {
        "assignee": null,
        "claim_note": "",
        "claimed_at": null,
        "created_at": "2026-02-24T01:34:34.323597Z",
        "depends_on": [
          "fn-60-run-eval-suite-against-skills-and-fix.7"
        ],
        "epic": "fn-60-run-eval-suite-against-skills-and-fix",
        "id": "fn-60-run-eval-suite-against-skills-and-fix.1",
        "priority": null,
        "spec_path": ".flow/tasks/fn-60-run-eval-suite-against-skills-and-fix.1.md",
        "status": "todo",
        "title": "Run initial eval suite and capture raw results",
        "updated_at": "2026-02-24T01:42:14.964788Z"
      },
      "id": "fn-60-run-eval-suite-against-skills-and-fix.1",
      "runtime": null,
      "spec": "# fn-60.1 Run initial eval suite and capture raw results\n\n## Description\n\nRun all 4 eval types against the current 131-skill catalog to establish the \"before\" state. This is the diagnostic baseline -- no fixes yet, just measurement.\n\n**Size:** M\n**Files:**\n- `tests/evals/results/` (output directory, gitignored)\n\n## Approach\n\nRun each eval type in sequence with real API calls (ANTHROPIC_API_KEY required):\n\n1. **L3 Activation**: `python3 tests/evals/run_activation.py` -- 73 cases, ~55 positive + 18 negative\n2. **L4 Confusion**: `python3 tests/evals/run_confusion_matrix.py` -- 36 confusion + 18 negative controls\n3. **L5 Effectiveness**: `python3 tests/evals/run_effectiveness.py --runs 3` -- 12 skills x 2 prompts x 3 runs\n4. **L6 Size Impact**: `python3 tests/evals/run_size_impact.py` -- 11 candidates, 36 comparisons\n\nExpected cost: ~$3-5 total (all haiku).\n\nSave all result JSON files. Record key metrics from each run's summary output for the analysis task.\n\n## Acceptance\n- [ ] All 4 eval runners complete without errors (exit 0)\n- [ ] Result JSON files exist in `tests/evals/results/` for all 4 eval types\n- [ ] Key metrics captured: activation TPR/FPR/accuracy, confusion per-group accuracy, effectiveness per-skill win rates, size impact comparison results\n- [ ] Total cost documented\n- [ ] `./scripts/validate-skills.sh && ./scripts/validate-marketplace.sh` still pass (no changes to skills yet)\n\n## Done summary\nTBD\n\n## Evidence\n- Commits:\n- Tests:\n- PRs:\n"
    },
    {
      "data": {
        "assignee": null,
        "claim_note": "",
        "claimed_at": null,
        "created_at": "2026-02-24T01:34:37.663196Z",
        "depends_on": [
          "fn-60-run-eval-suite-against-skills-and-fix.1"
        ],
        "epic": "fn-60-run-eval-suite-against-skills-and-fix",
        "id": "fn-60-run-eval-suite-against-skills-and-fix.2",
        "priority": null,
        "spec_path": ".flow/tasks/fn-60-run-eval-suite-against-skills-and-fix.2.md",
        "status": "todo",
        "title": "Analyze results and triage findings",
        "updated_at": "2026-02-24T01:36:27.187299Z"
      },
      "id": "fn-60-run-eval-suite-against-skills-and-fix.2",
      "runtime": null,
      "spec": "# fn-60.2 Analyze results and triage findings\n\n## Description\n\nAnalyze the raw results from task .1 across all 4 eval types. Identify worst performers, common failure patterns, and produce a prioritized fix list. This analysis drives tasks .3 and .4.\n\n**Size:** M\n**Files:**\n- No file changes -- analysis only, findings documented in this task's done summary\n\n## Approach\n\nFor each eval type, extract and document:\n\n### L3 Activation\n- Overall TPR, FPR, accuracy vs thresholds (TPR>=75%, FPR<=20%, accuracy>=70%)\n- Per-skill activation rates -- which skills are never found?\n- Detection method breakdown -- how many structured vs fallback vs parse_failure?\n- False positives -- which negative controls failed?\n\n### L4 Confusion\n- Per-group accuracy vs threshold (>=60%)\n- Cross-activation rate per group vs threshold (<=35%)\n- Flagged cross-activation pairs (>20% rate)\n- Low-discrimination skills and \"never-activated\" skills\n- Negative control pass rate vs threshold (>=70%)\n\n### L5 Effectiveness\n- Per-skill win rates vs threshold (overall >=50%)\n- Mean improvement per skill -- any negative (skill makes things worse)?\n- Error rates -- any skills with generation/judge failures?\n- Per-criterion breakdown -- which criteria drive wins/losses?\n\n### L6 Size Impact\n- full vs baseline win rate vs threshold (>=55%)\n- Any skills where baseline beats full?\n- full vs summary comparison -- is full always better?\n- Size tier correlations -- do large skills benefit more from full content?\n\n### Triage Output\n- Prioritized list of skills to fix (worst first)\n- Classification: description problem (L3/L4) vs content problem (L5)\n- Estimated fix complexity per skill\n\n## Acceptance\n- [ ] All 4 eval type results analyzed\n- [ ] Current metrics vs quality bar thresholds documented\n- [ ] Prioritized fix list with classification (description vs content)\n- [ ] Fix list informs task .3 (routing fixes) and task .4 (content fixes)\n- [ ] Any threshold adjustments documented with rationale\n\n## Done summary\nTBD\n\n## Evidence\n- Commits:\n- Tests:\n- PRs:\n"
    },
    {
      "data": {
        "assignee": null,
        "claim_note": "",
        "claimed_at": null,
        "created_at": "2026-02-24T01:34:41.008238Z",
        "depends_on": [
          "fn-60-run-eval-suite-against-skills-and-fix.2"
        ],
        "epic": "fn-60-run-eval-suite-against-skills-and-fix",
        "id": "fn-60-run-eval-suite-against-skills-and-fix.3",
        "priority": null,
        "spec_path": ".flow/tasks/fn-60-run-eval-suite-against-skills-and-fix.3.md",
        "status": "todo",
        "title": "Fix activation and confusion routing issues",
        "updated_at": "2026-02-24T01:36:31.988305Z"
      },
      "id": "fn-60-run-eval-suite-against-skills-and-fix.3",
      "runtime": null,
      "spec": "# fn-60.3 Fix activation and confusion routing issues\n\n## Description\n\nBased on the analysis from task .2, fix skill descriptions (frontmatter) to improve activation routing accuracy and reduce confusion between overlapping skills.\n\n**Size:** L\n**Files:**\n- `skills/*/SKILL.md` -- frontmatter description updates for skills identified in .2\n\n## Approach\n\n### Activation Fixes\n- Skills that were never activated in L3: make descriptions more specific and action-oriented\n- Skills with low activation rates: add key differentiating terms to descriptions\n- Follow the routing style guide: Action + Domain + Differentiator formula (docs/skill-routing-style-guide.md)\n- Keep descriptions under 120 characters\n\n### Confusion Fixes\n- For flagged cross-activation pairs (>20% rate): differentiate the two skills' descriptions\n- For low-discrimination skills: add unique qualifying terms\n- Focus on the 7 domain groups: testing, security, data, performance, api, cicd, blazor\n- Ensure each skill in a group has a clearly distinct \"action\" or \"domain\" keyword\n\n### Validation\n- Run `./scripts/validate-skills.sh` after each batch of changes\n- Quick-verify with `python3 tests/evals/run_activation.py --dry-run` (index char count should stay reasonable)\n- Spot-check individual skills with `python3 tests/evals/run_activation.py --skill <name>`\n\n## Acceptance\n- [ ] All skills identified in .2 analysis have updated descriptions\n- [ ] Descriptions still under 120 characters\n- [ ] Total description budget still under 15,600 characters\n- [ ] `./scripts/validate-skills.sh && ./scripts/validate-marketplace.sh` pass\n- [ ] Changes follow routing style guide conventions\n\n## Done summary\nTBD\n\n## Evidence\n- Commits:\n- Tests:\n- PRs:\n"
    },
    {
      "data": {
        "assignee": null,
        "claim_note": "",
        "claimed_at": null,
        "created_at": "2026-02-24T01:34:44.467374Z",
        "depends_on": [
          "fn-60-run-eval-suite-against-skills-and-fix.2"
        ],
        "epic": "fn-60-run-eval-suite-against-skills-and-fix",
        "id": "fn-60-run-eval-suite-against-skills-and-fix.4",
        "priority": null,
        "spec_path": ".flow/tasks/fn-60-run-eval-suite-against-skills-and-fix.4.md",
        "status": "todo",
        "title": "Fix effectiveness issues for low-scoring skills",
        "updated_at": "2026-02-24T01:36:36.863216Z"
      },
      "id": "fn-60-run-eval-suite-against-skills-and-fix.4",
      "runtime": null,
      "spec": "# fn-60.4 Fix effectiveness issues for low-scoring skills\n\n## Description\n\nBased on the analysis from task .2, improve skill content for skills where the enhanced (with-skill) output loses to or ties with the baseline (without-skill) output in L5 effectiveness evals.\n\n**Size:** M\n**Files:**\n- `skills/*/SKILL.md` -- content improvements for underperforming skills (body, not frontmatter)\n\n## Approach\n\n### Content Fixes\n- For skills with 0% win rate: investigate what the judge criteria expect vs what the skill provides\n- For skills with <50% win rate: identify which criteria the skill loses on (per_criterion_breakdown)\n- Common patterns to check:\n  - Skill content is too generic (not specific enough for the eval prompts)\n  - Skill content is outdated (references old APIs or patterns)\n  - Skill content lacks concrete examples that would help generation\n  - Scope section doesn't cover the test prompt topic\n\n### Fix Strategy\n- Prioritize skills where wins_enhanced=0 (total failures)\n- For partial failures: check which rubric criteria drive the losses\n- Add or improve examples, code patterns, or specific guidance\n- Do NOT rewrite entire skills -- targeted improvements only\n\n### Validation\n- Spot-check individual skills: `python3 tests/evals/run_effectiveness.py --skill <name> --runs 3 --regenerate`\n- `./scripts/validate-skills.sh` after changes\n\n## Acceptance\n- [ ] All skills with 0% win rate from .2 analysis have been investigated and improved\n- [ ] Skills with <50% win rate have targeted content improvements\n- [ ] `./scripts/validate-skills.sh && ./scripts/validate-marketplace.sh` pass\n- [ ] Individual skill re-runs show improvement (documented in done summary)\n\n## Done summary\nTBD\n\n## Evidence\n- Commits:\n- Tests:\n- PRs:\n"
    },
    {
      "data": {
        "assignee": null,
        "claim_note": "",
        "claimed_at": null,
        "created_at": "2026-02-24T01:34:48.403802Z",
        "depends_on": [
          "fn-60-run-eval-suite-against-skills-and-fix.3",
          "fn-60-run-eval-suite-against-skills-and-fix.4"
        ],
        "epic": "fn-60-run-eval-suite-against-skills-and-fix",
        "id": "fn-60-run-eval-suite-against-skills-and-fix.5",
        "priority": null,
        "spec_path": ".flow/tasks/fn-60-run-eval-suite-against-skills-and-fix.5.md",
        "status": "todo",
        "title": "Rerun eval suite and validate against quality bar",
        "updated_at": "2026-02-24T01:36:42.245050Z"
      },
      "id": "fn-60-run-eval-suite-against-skills-and-fix.5",
      "runtime": null,
      "spec": "# fn-60.5 Rerun eval suite and validate against quality bar\n\n## Description\n\nFull rerun of all 4 eval types after fixes from tasks .3 and .4. Validate results against the quality bar thresholds defined in the epic spec. If thresholds are not met, do one more targeted fix-and-rerun iteration.\n\n**Size:** M\n**Files:**\n- `tests/evals/results/` (output directory, gitignored)\n\n## Approach\n\n1. Run all 4 eval types with `--regenerate` flag (where applicable) to ensure fresh generations:\n   - `python3 tests/evals/run_activation.py`\n   - `python3 tests/evals/run_confusion_matrix.py`\n   - `python3 tests/evals/run_effectiveness.py --runs 3 --regenerate`\n   - `python3 tests/evals/run_size_impact.py --regenerate`\n\n2. Compare results against quality bar thresholds:\n   - L3: TPR>=75%, FPR<=20%, accuracy>=70%\n   - L4: per-group accuracy>=60%, cross-activation<=35%, negative control pass rate>=70%\n   - L5: overall win rate>=50%, mean improvement>0, no skill at 0% win rate\n   - L6: full>baseline in >=55% of comparisons\n\n3. If any threshold is missed:\n   - Identify the specific failures\n   - Make targeted fixes (another iteration of .3/.4 work)\n   - Rerun the failing eval type only\n   - Document the iteration in the done summary\n\n4. If a threshold proves systematically unachievable (e.g., haiku consistently can't route to certain skills), document the rationale and adjust the threshold.\n\n## Acceptance\n- [ ] All 4 eval types re-run with fresh results\n- [ ] Results compared against quality bar thresholds\n- [ ] All thresholds met OR documented rationale for exceptions\n- [ ] Before/after comparison documented (initial run vs final run)\n- [ ] Total cost across all iterations documented\n- [ ] `./scripts/validate-skills.sh && ./scripts/validate-marketplace.sh` still pass\n\n## Done summary\nTBD\n\n## Evidence\n- Commits:\n- Tests:\n- PRs:\n"
    },
    {
      "data": {
        "assignee": null,
        "claim_note": "",
        "claimed_at": null,
        "created_at": "2026-02-24T01:34:51.977858Z",
        "depends_on": [
          "fn-60-run-eval-suite-against-skills-and-fix.5"
        ],
        "epic": "fn-60-run-eval-suite-against-skills-and-fix",
        "id": "fn-60-run-eval-suite-against-skills-and-fix.6",
        "priority": null,
        "spec_path": ".flow/tasks/fn-60-run-eval-suite-against-skills-and-fix.6.md",
        "status": "todo",
        "title": "Save initial baselines and unblock fn-58.4",
        "updated_at": "2026-02-24T01:36:48.726254Z"
      },
      "id": "fn-60-run-eval-suite-against-skills-and-fix.6",
      "runtime": null,
      "spec": "# fn-60.6 Save initial baselines and unblock fn-58.4\n\n## Description\n\nSave the passing eval results as initial baseline JSON files in `tests/evals/baselines/`. These baselines are used by `compare_baseline.py` for future regression detection. Then add this task as a dependency on fn-58.4 (CI workflow task).\n\n**Size:** S\n**Files:**\n- `tests/evals/baselines/effectiveness_baseline.json` (new)\n- `tests/evals/baselines/activation_baseline.json` (new)\n- `tests/evals/baselines/confusion_baseline.json` (new)\n- `tests/evals/baselines/size_impact_baseline.json` (new)\n\n## Approach\n\n1. Extract the `summary` object from each eval type's result JSON\n2. Save with a small `meta` header (run_id, timestamp) as per fn-58.4 spec\n3. Baseline format: `{\"meta\": {...}, \"summary\": {...}}`\n4. `compare_baseline.py` ignores `cases` and `artifacts` fields -- only `summary` matters\n5. Verify baseline files work with: `python3 tests/evals/compare_baseline.py --eval-type <type>`\n6. Replace `.gitkeep` files in `baselines/` directory with real files\n7. fn-58.4 already has this task listed as a dependency (added during epic creation)\n\n## Acceptance\n- [ ] All 4 baseline JSON files exist in `tests/evals/baselines/`\n- [ ] Each baseline contains `meta` and `summary` keys\n- [ ] `compare_baseline.py` runs without error against all 4 baseline types\n- [ ] fn-58.4 has dependency on this task\n- [ ] `./scripts/validate-skills.sh && ./scripts/validate-marketplace.sh` pass\n\n## Done summary\nTBD\n\n## Evidence\n- Commits:\n- Tests:\n- PRs:\n"
    },
    {
      "data": {
        "assignee": null,
        "claim_note": "",
        "claimed_at": null,
        "created_at": "2026-02-24T01:42:09.523578Z",
        "depends_on": [],
        "epic": "fn-60-run-eval-suite-against-skills-and-fix",
        "id": "fn-60-run-eval-suite-against-skills-and-fix.7",
        "priority": 0,
        "spec_path": ".flow/tasks/fn-60-run-eval-suite-against-skills-and-fix.7.md",
        "status": "todo",
        "title": "Fix eval auth and add skill restore safety",
        "updated_at": "2026-02-24T01:42:34.666691Z"
      },
      "id": "fn-60-run-eval-suite-against-skills-and-fix.7",
      "runtime": null,
      "spec": "# fn-60.7 Fix eval auth and add skill restore safety\n\n## Description\n\nTwo infrastructure fixes before running evals:\n\n1. **Auth**: `_common.py:get_client()` hard-requires `ANTHROPIC_API_KEY` env var. This breaks on machines where the Anthropic SDK picks up auth via default mechanisms (e.g., key in shell profile, SDK auto-discovery). Fix: let the SDK handle auth discovery instead of pre-validating.\n\n2. **Skill restore**: Tasks .3 and .4 will modify skill SKILL.md files. Need a safe restore mechanism so originals can be recovered. Approach: git-based -- all modifications happen on a dedicated branch, and `git checkout -- skills/` restores originals. Document this in task specs.\n\n**Size:** S\n**Files:**\n- `tests/evals/_common.py` -- fix `get_client()` to not hard-require ANTHROPIC_API_KEY\n- `tests/evals/config.yaml` -- update comment about auth\n\n## Approach\n\n### Auth Fix\n\nChange `get_client()` in `_common.py`:\n- If an explicit `api_key` is passed, use it\n- If `ANTHROPIC_API_KEY` env var is set, use it\n- Otherwise, create `anthropic.Anthropic()` with no key arg -- let the SDK handle auth discovery\n- Remove the pre-validation `ValueError` -- let the SDK raise its own error if auth fails\n\nThis makes the runners work with:\n- `ANTHROPIC_API_KEY` env var (CI, explicit)\n- SDK default auth (local development)\n- Explicit key parameter (programmatic)\n\n### Skill Restore Strategy\n\nThe eval runners (`run_*.py`) only READ skills from `REPO_ROOT/skills/` -- they never modify them. Skills are modified only in tasks .3 and .4 (manual edits to SKILL.md files).\n\nRestore mechanism:\n- All changes happen on the current branch (`evals`)\n- At any point, `git checkout -- skills/` restores all skill files to their committed state\n- Before starting fix tasks (.3/.4), commit the current clean state as a checkpoint\n- After each fix batch, commit with a descriptive message\n- If a fix batch makes things worse, `git revert` the commit\n\nThis mirrors test.sh's snapshot/restore pattern but uses git instead of rsync.\n\n## Acceptance\n- [ ] `get_client()` works without `ANTHROPIC_API_KEY` env var when SDK has default auth\n- [ ] `get_client()` still works WITH `ANTHROPIC_API_KEY` env var (backward compat)\n- [ ] Config comment updated to reflect auth flexibility\n- [ ] `pip install -r tests/evals/requirements.txt` succeeds (anthropic SDK installable)\n- [ ] `python3 tests/evals/run_activation.py --dry-run` still works\n\n## Done summary\nTBD\n\n## Evidence\n- Commits:\n- Tests:\n- PRs:\n"
    }
  ]
}

{
  "created_at": "2026-02-23T21:23:02.464134Z",
  "epic": {
    "data": {
      "branch_name": "fn-58-offline-skill-evaluation-framework-from",
      "completion_review_status": "unknown",
      "completion_reviewed_at": null,
      "created_at": "2026-02-23T21:17:17.113074Z",
      "default_impl": null,
      "default_review": null,
      "default_sync": null,
      "depends_on_epics": [],
      "id": "fn-58-offline-skill-evaluation-framework-from",
      "next_task": 1,
      "plan_review_status": "unknown",
      "plan_reviewed_at": null,
      "spec_path": ".flow/specs/fn-58-offline-skill-evaluation-framework-from.md",
      "status": "open",
      "title": "Offline Skill Evaluation Framework from dotnet-skills-evals",
      "updated_at": "2026-02-23T21:22:17.901457Z"
    },
    "spec": "# Offline Skill Evaluation Framework from dotnet-skills-evals\n\n## Overview\n\nAdopt the best evaluation patterns from [Aaronontheweb/dotnet-skills-evals](https://github.com/Aaronontheweb/dotnet-skills-evals), [sjnims/cc-plugin-eval](https://github.com/sjnims/cc-plugin-eval), and [Anthropic's eval guidance](https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents) to build a comprehensive offline evaluation framework for dotnet-artisan's 131 skills.\n\nToday the plugin validates skill **activation** (does the right skill load?) via structural checks, copilot smoke tests, and live routing tests. This epic adds three missing dimensions:\n\n1. **Effectiveness** \u2014 Does loading a skill actually improve output quality? (A/B comparison with LLM judge)\n2. **Offline activation** \u2014 Can models correctly route to skills without CLI invocation? (API-only, cheap, fast)\n3. **Size impact** \u2014 Does skill content format (full vs summary vs none) affect quality? (Progressive disclosure validation)\n4. **Confusion matrix** \u2014 Do overlapping skills cause misrouting? (Disambiguation quality at 131-skill scale)\n\n### Eval Layer Architecture (after this epic)\n\n```\nL0: Structural validation (validate-skills.sh)              \u2190 EXISTS\nL1: Activation smoke tests (copilot-smoke/run_smoke.py)      \u2190 EXISTS  \nL2: Live routing + evidence tiers (check-skills.cs)          \u2190 EXISTS\nL3: Offline activation evals (run_activation.py)             \u2190 NEW\nL4: Confusion matrix / disambiguation (run_confusion_matrix) \u2190 NEW\nL5: Effectiveness evals (run_effectiveness.py)               \u2190 NEW\nL6: Size impact evals (run_size_impact.py)                   \u2190 NEW\n```\n\n## Scope\n\n**In scope \u2014 everything:**\n- Offline effectiveness eval runner (Python, A/B comparison + LLM judge)\n- Per-skill rubric YAML schema and rubric files (10-15 priority skills to start)\n- Offline activation eval with JSONL datasets (50+ positive cases, 15+ negative controls)\n- Confusion matrix tests for 7+ domain groups of overlapping skills\n- Size impact evals (full vs summary vs baseline, with sibling file testing)\n- CI workflow for scheduled evals (weekly)\n- Baseline regression tracking for all eval types\n- Full documentation updates across all affected files\n- Cost controls (model selection, dry-run mode, batching, skip-if-no-rubric)\n\n## Approach\n\n### Stack\n\nPython (consistent with existing `run_smoke.py`, `_validate_skills.py`). Direct Anthropic SDK API calls \u2014 no DSPy or heavy ML framework dependencies. YAML rubrics, JSONL datasets, JSON results.\n\n### Key Design Decisions\n\n1. **Diagnostic first, CI gate later** \u2014 All evals run on schedule (weekly) and produce reports. They do NOT block PRs initially because LLM-judged scores are non-deterministic. Gate can be added once baseline stability is proven.\n\n2. **Layered detection** \u2014 Programmatic exact-match first, LLM fallback second (following cc-plugin-eval's `programmatic_first` pattern). Avoids the substring false-positive problem found in dotnet-skills-evals.\n\n3. **Compressed routing index** \u2014 Offline activation builds the index dynamically from skill frontmatter. The reference repo proved compressed index outperforms fat index (56.5% TPR, 0% FPR, 84.6% accuracy).\n\n4. **Randomized A/B** \u2014 All pairwise comparisons randomize output ordering to avoid position bias (documented at 80.2% selection rate for first-listed in the literature).\n\n5. **Cost-controlled** \u2014 Haiku for iteration, Sonnet for final scoring. Dry-run mode for dataset development. Skip skills without rubrics. Target: <$15 per full suite run.\n\n### Eval Flow\n\n```mermaid\ngraph TD\n    subgraph \"L3: Activation\"\n        A1[Build compressed index from skills/] --> A2[Present prompt + index to model]\n        A2 --> A3[Detect skill reference: exact match \u2192 LLM fallback]\n        A3 --> A4[Record TPR / FPR / accuracy]\n    end\n    \n    subgraph \"L4: Confusion Matrix\"\n        C1[Group overlapping skills by domain] --> C2[Present ambiguous prompts]\n        C2 --> C3[Track which skills activate per group]\n        C3 --> C4[Build NxN confusion matrix per group]\n    end\n    \n    subgraph \"L5: Effectiveness\"\n        E1[Load rubric + test prompt] --> E2[Generate WITH skill]\n        E1 --> E3[Generate WITHOUT skill]\n        E2 --> E4[Randomize A/B]\n        E3 --> E4\n        E4 --> E5[Judge per rubric criteria]\n        E5 --> E6[Win rate + mean improvement]\n    end\n    \n    subgraph \"L6: Size Impact\"\n        S1[Select candidate skill] --> S2[Full / Summary / Baseline]\n        S2 --> S3[Pairwise judge comparisons]\n        S3 --> S4[Identify optimal content level]\n    end\n```\n\n## Quick commands\n\n```bash\n# Activation eval (offline, API-only)\npython tests/evals/run_activation.py\npython tests/evals/run_activation.py --skill dotnet-xunit --dry-run\n\n# Effectiveness eval (A/B with LLM judge)\npython tests/evals/run_effectiveness.py\npython tests/evals/run_effectiveness.py --skill dotnet-xunit --runs 3\n\n# Size impact eval\npython tests/evals/run_size_impact.py\n\n# Confusion matrix analysis\npython tests/evals/run_confusion_matrix.py\npython tests/evals/run_confusion_matrix.py --group testing\n\n# Baseline comparison (any eval type)\npython tests/evals/compare_baseline.py\n\n# Validate rubric schemas\npython tests/evals/validate_rubrics.py\n\n# Dry run everything\npython tests/evals/run_activation.py --dry-run\npython tests/evals/run_effectiveness.py --dry-run\npython tests/evals/run_size_impact.py --dry-run\n```\n\n## Acceptance\n\n- [ ] `tests/evals/` directory with all runners, configs, datasets, rubrics, and baselines\n- [ ] Offline activation eval: 50+ positive cases, 15+ negative controls, TPR/FPR/accuracy metrics\n- [ ] Effectiveness eval: 10+ skills with rubrics, A/B comparison, win rate + mean improvement\n- [ ] Size impact eval: 8+ skills tested at Full/Summary/Baseline, pairwise comparisons\n- [ ] Confusion matrix: 7+ domain groups, NxN activation matrices, cross-activation flagging\n- [ ] All evals support `--dry-run`, `--skill <name>`, and produce JSON results\n- [ ] GitHub Actions workflow runs all evals on weekly schedule\n- [ ] Baseline regression tracking detects quality drops\n- [ ] Documentation updated: CONTRIBUTING-SKILLS.md, AGENTS.md, docs/skill-eval-framework.md, README.md, CHANGELOG.md\n- [ ] Full eval suite cost documented and under $15 per run\n- [ ] `./scripts/validate-skills.sh && ./scripts/validate-marketplace.sh` still pass\n\n## Risks\n\n| Risk | Mitigation |\n|------|------------|\n| LLM judge non-determinism | temperature=0.0, 3 trials per case, report mean + stddev |\n| Cost at 131-skill scale | Skip skills without rubrics; Haiku for iteration; batch support |\n| Rubric quality varies | Template + schema validation; review rubrics in PRs |\n| API outages during scheduled runs | Retry with backoff; partial results are valid |\n| Self-evaluation bias | Configurable judge model; document when same model family used |\n| Substring false positives in activation | Layered detection: exact match first, LLM fallback second |\n| Confusion matrix scale (131 x 131) | Test within domain groups (5-7 skills), not all pairs |\n\n## References\n\n- [Aaronontheweb/dotnet-skills-evals](https://github.com/Aaronontheweb/dotnet-skills-evals) \u2014 primary reference: activation, effectiveness, size impact evals\n- [sjnims/cc-plugin-eval](https://github.com/sjnims/cc-plugin-eval) \u2014 programmatic_first detection, session batching, safety patterns\n- [Anthropic: Demystifying Evals](https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents) \u2014 canonical eval guidance\n- [Agent Skills Best Practices](https://platform.claude.com/docs/en/agents-and-tools/agent-skills/best-practices) \u2014 official skill eval recommendations\n- [OpenAI: Testing Agent Skills with Evals](https://developers.openai.com/blog/eval-skills/) \u2014 routing eval patterns\n- [Tool Selection Bias (arXiv 2505.18135)](https://arxiv.org/abs/2505.18135) \u2014 assertive cue bias in tool descriptions\n- Existing infrastructure: `tests/copilot-smoke/`, `tests/agent-routing/`, `scripts/_validate_skills.py`\n- Related completed epics: fn-54 (routing harness), fn-55 (invocation contracts), fn-57 (copilot testing)\n"
  },
  "epic_id": "fn-58-offline-skill-evaluation-framework-from",
  "schema_version": 2,
  "tasks": [
    {
      "data": {
        "assignee": null,
        "claim_note": "",
        "claimed_at": null,
        "created_at": "2026-02-23T21:18:06.266571Z",
        "depends_on": [],
        "epic": "fn-58-offline-skill-evaluation-framework-from",
        "id": "fn-58-offline-skill-evaluation-framework-from.1",
        "priority": null,
        "spec_path": ".flow/tasks/fn-58-offline-skill-evaluation-framework-from.1.md",
        "status": "todo",
        "title": "Scaffold eval directory, rubric schema, and runner skeleton",
        "updated_at": "2026-02-23T21:18:41.621383Z"
      },
      "id": "fn-58-offline-skill-evaluation-framework-from.1",
      "runtime": null,
      "spec": "# fn-58-offline-skill-evaluation-framework-from.1 Scaffold eval directory, rubric schema, and runner skeleton\n\n## Description\nScaffold the `tests/evals/` directory structure, define the rubric YAML schema, create the eval configuration, and build the skeleton Python runner that the effectiveness eval (task .3) will flesh out.\n\n**Size:** M\n**Files:**\n- `tests/evals/run_effectiveness.py` (skeleton runner with CLI args, config loading, dry-run mode)\n- `tests/evals/compare_baseline.py` (baseline comparison utility)\n- `tests/evals/config.yaml` (eval configuration: models, temperature, retry policy, cost limits)\n- `tests/evals/rubric_schema.yaml` (JSON Schema for rubric YAML files)\n- `tests/evals/validate_rubrics.py` (rubric schema validation script)\n- `tests/evals/rubrics/` (empty directory, populated by task .2)\n- `tests/evals/baselines/` (empty directory for baseline JSON files)\n- `tests/evals/requirements.txt` (Python dependencies)\n\n## Approach\n\n- Follow the pattern from `dotnet-skills-evals/datasets/rubrics/*.yaml` for rubric schema: `skill_name`, `criteria[]` with `name`, `weight`, `description`\n- Runner skeleton follows the pattern of `tests/copilot-smoke/run_smoke.py:1` \u2014 argparse CLI with `--skill`, `--dry-run`, `--model`, `--judge-model` flags\n- Config file follows the convention of existing test infrastructure (YAML, not JSON) for human-editability\n- `validate_rubrics.py` follows the pattern of `scripts/_validate_skills.py:1` \u2014 exits non-zero on validation failure for CI integration\n- Rubric schema should validate: required fields present, weights sum to 1.0, no empty descriptions, skill_name matches filename\n\n## Key context\n\n- The reference implementation uses DSPy signatures for structured LLM calls. We use direct API calls instead (simpler dependency chain). The runner skeleton should define clear function signatures that task .3 will implement.\n- Existing Python deps are minimal (see `tests/copilot-smoke/run_smoke.py` \u2014 only stdlib). New deps should be limited to `anthropic` SDK and `pyyaml`.\n- The config should support `ANTHROPIC_API_KEY` and optionally `OPENROUTER_API_KEY` env vars.\n## Acceptance\n- [ ] `tests/evals/` directory exists with all listed files\n- [ ] `rubric_schema.yaml` defines required fields: `skill_name` (string), `criteria` (array of objects with `name`, `weight`, `description`)\n- [ ] `validate_rubrics.py` exits 0 with no rubrics, exits non-zero for malformed rubric YAML\n- [ ] `run_effectiveness.py --dry-run` lists skills with rubrics and exits 0 without API calls\n- [ ] `run_effectiveness.py --help` shows all CLI flags (--skill, --dry-run, --model, --judge-model, --runs)\n- [ ] `config.yaml` includes model defaults, temperature (0.0), max retries, and cost-per-run estimate\n- [ ] `requirements.txt` includes only `anthropic` and `pyyaml` (minimal deps)\n- [ ] `validate-skills.sh` or a new validation step can invoke `validate_rubrics.py`\n## Done summary\nTBD\n\n## Evidence\n- Commits:\n- Tests:\n- PRs:\n"
    },
    {
      "data": {
        "assignee": null,
        "claim_note": "",
        "claimed_at": null,
        "created_at": "2026-02-23T21:18:10.626674Z",
        "depends_on": [
          "fn-58-offline-skill-evaluation-framework-from.1"
        ],
        "epic": "fn-58-offline-skill-evaluation-framework-from",
        "id": "fn-58-offline-skill-evaluation-framework-from.2",
        "priority": null,
        "spec_path": ".flow/tasks/fn-58-offline-skill-evaluation-framework-from.2.md",
        "status": "todo",
        "title": "Author priority rubric YAML files for 10-15 skills",
        "updated_at": "2026-02-23T21:19:04.575528Z"
      },
      "id": "fn-58-offline-skill-evaluation-framework-from.2",
      "runtime": null,
      "spec": "# fn-58-offline-skill-evaluation-framework-from.2 Author priority rubric YAML files for 10-15 skills\n\n## Description\nAuthor rubric YAML files for 10-15 priority skills, following the schema defined in task .1. Rubrics define per-criterion scoring guidelines that the LLM judge uses to compare skill-loaded vs baseline code output.\n\n**Size:** M\n**Files:**\n- `tests/evals/rubrics/dotnet-xunit.yaml`\n- `tests/evals/rubrics/dotnet-minimal-apis.yaml`\n- `tests/evals/rubrics/dotnet-efcore-patterns.yaml`\n- `tests/evals/rubrics/dotnet-csharp-coding-standards.yaml`\n- `tests/evals/rubrics/dotnet-csharp-async-patterns.yaml`\n- `tests/evals/rubrics/dotnet-resilience.yaml`\n- `tests/evals/rubrics/dotnet-containers.yaml`\n- `tests/evals/rubrics/dotnet-blazor-patterns.yaml`\n- `tests/evals/rubrics/dotnet-testing-strategy.yaml`\n- `tests/evals/rubrics/dotnet-observability.yaml`\n- `tests/evals/rubrics/dotnet-security-owasp.yaml`\n- `tests/evals/rubrics/dotnet-native-aot.yaml`\n- Additional rubrics as time allows (target 15)\n\nEach rubric also includes 1-2 test prompts (realistic developer questions) that serve as the eval input.\n\n## Approach\n\n- Follow the rubric format from `dotnet-skills-evals/datasets/rubrics/*.yaml` \u2014 each rubric has 3-6 weighted criteria\n- Read the corresponding `skills/<skill-name>/SKILL.md` to understand what the skill teaches, then write criteria that test whether an LLM *using* the skill produces output aligned with the skill's guidance\n- Criteria weights should sum to 1.0 and reflect the relative importance of each aspect\n- Test prompts should be realistic developer questions, not quiz-style (\"how to X with Y in .NET\")\n- Prioritization rationale:\n  - **User-invocable + code-producing skills** \u2014 these have clear A/B testability\n  - **High overlap risk** \u2014 xunit, testing-strategy, efcore-patterns overlap and rubrics disambiguate value\n  - **Broad usage** \u2014 coding-standards, async-patterns are loaded frequently via advisor routing\n  - **Specialized knowledge** \u2014 resilience, native-aot, observability add knowledge the model may not have natively\n\n## Key context\n\n- The reference implementation found that **specialized/niche skills benefit most** from rubrics (+2.33 improvement) while **general-knowledge skills show weaker improvement** (+0.33). Expect rubrics for skills like `dotnet-native-aot` and `dotnet-resilience` to show the strongest A/B signal.\n- Non-code skills (dotnet-advisor, dotnet-ui-chooser, dotnet-agent-gotchas) are excluded \u2014 they produce routing/guidance, not code, and require a different eval methodology.\n- Each rubric should include a `test_prompts` field (array of strings) so the effectiveness runner can self-serve test inputs without a separate dataset file.\n## Acceptance\n- [ ] At least 10 rubric YAML files exist under `tests/evals/rubrics/`\n- [ ] All rubrics pass `python tests/evals/validate_rubrics.py` (schema validation from task .1)\n- [ ] Each rubric has 3-6 criteria with weights summing to 1.0 (tolerance \u00b10.01)\n- [ ] Each rubric has at least 1 test prompt in the `test_prompts` field\n- [ ] Criteria descriptions reference specific patterns/APIs the skill teaches (not generic \"good code\" statements)\n- [ ] No rubric references a skill that does not exist in `skills/` directory\n- [ ] Rubric file names match the skill directory names exactly\n## Done summary\nTBD\n\n## Evidence\n- Commits:\n- Tests:\n- PRs:\n"
    },
    {
      "data": {
        "assignee": null,
        "claim_note": "",
        "claimed_at": null,
        "created_at": "2026-02-23T21:18:14.173270Z",
        "depends_on": [
          "fn-58-offline-skill-evaluation-framework-from.1"
        ],
        "epic": "fn-58-offline-skill-evaluation-framework-from",
        "id": "fn-58-offline-skill-evaluation-framework-from.3",
        "priority": null,
        "spec_path": ".flow/tasks/fn-58-offline-skill-evaluation-framework-from.3.md",
        "status": "todo",
        "title": "Build A/B effectiveness eval runner with LLM judge",
        "updated_at": "2026-02-23T21:19:30.078392Z"
      },
      "id": "fn-58-offline-skill-evaluation-framework-from.3",
      "runtime": null,
      "spec": "# fn-58-offline-skill-evaluation-framework-from.3 Build A/B effectiveness eval runner with LLM judge\n\n## Description\nImplement the A/B effectiveness eval runner that compares code generated with skill content injected vs code generated without it, using an LLM judge to score both outputs against the rubric criteria.\n\n**Size:** M\n**Files:**\n- `tests/evals/run_effectiveness.py` (flesh out the skeleton from task .1)\n- `tests/evals/judge_prompt.py` (LLM judge prompt templates and response parsing)\n- `tests/evals/results/` (output directory for eval results JSON)\n\n## Approach\n\n- For each skill with a rubric:\n  1. Read the skill SKILL.md content\n  2. For each test prompt in the rubric:\n     a. **Enhanced run**: System prompt includes the full SKILL.md content, user prompt is the test prompt \u2192 generate code\n     b. **Baseline run**: System prompt is generic (\"You are a .NET developer\"), same user prompt \u2192 generate code\n  3. **Judge step**: Randomize A/B ordering, present both outputs + rubric criteria to judge model, collect per-criterion scores (1-5 scale)\n  4. **Score**: Compute per-criterion winner, overall winner, mean improvement\n\n- Follow the pattern of `tests/copilot-smoke/run_smoke.py:1` for result output format: JSON with per-case results, summary statistics, and metadata (model, timestamp, run_id)\n- Judge prompt must:\n  - Present both outputs with randomized labels (Response A / Response B)\n  - Include the rubric criteria with descriptions\n  - Request structured JSON output: `{\"criteria\": [{\"name\": \"...\", \"score_a\": N, \"score_b\": N, \"reasoning\": \"...\"}], \"overall_winner\": \"A|B|tie\"}`\n  - Use a different system prompt than the generation model to reduce self-evaluation bias\n\n- Handle edge cases:\n  - Empty/refusal generation \u2192 score as 0, mark as `infra_error`\n  - Unparseable judge output \u2192 retry once, then mark as `judge_error`\n  - API rate limits \u2192 exponential backoff with jitter, configurable max retries from `config.yaml`\n\n## Key context\n\n- The reference implementation uses DSPy signatures for structured extraction. We use direct JSON-mode API calls instead: `response_format={\"type\": \"json_object\"}` with the Anthropic SDK does NOT exist \u2014 instead, include \"respond in JSON\" in the system prompt and parse the response.\n- Use `temperature=0.0` for both generation and judging (reproducibility). The reference implementation does this.\n- The reference implementation found position bias in judge outputs. Randomizing A/B ordering is critical \u2014 track which position the enhanced output was in for bias analysis.\n- Cost estimate: ~$0.02-0.05 per case (2 generations + 1 judge call with Haiku). For 15 rubrics \u00d7 2 prompts \u00d7 3 trials = 90 eval runs \u2248 $2-5 per full suite.\n## Acceptance\n- [ ] `run_effectiveness.py` generates code for both enhanced (with skill) and baseline (without skill) conditions\n- [ ] A/B ordering is randomized per case and recorded in results for bias analysis\n- [ ] LLM judge produces per-criterion scores (1-5) for both outputs\n- [ ] Results JSON includes: skill_name, prompt, enhanced_score, baseline_score, winner, per_criterion_breakdown, model, judge_model, run_id, timestamp\n- [ ] `--dry-run` mode shows planned eval cases without making API calls\n- [ ] `--skill <name>` filters to a single skill's rubric\n- [ ] `--runs N` controls number of trials per case (default 3)\n- [ ] API errors (rate limits, timeouts) are handled with retry + exponential backoff\n- [ ] Unparseable judge output triggers one retry, then records `judge_error`\n- [ ] Summary statistics printed: overall win rate, mean improvement, per-skill breakdown\n- [ ] Full eval run with 10+ rubrics completes and produces valid results JSON\n## Done summary\nTBD\n\n## Evidence\n- Commits:\n- Tests:\n- PRs:\n"
    },
    {
      "data": {
        "assignee": null,
        "claim_note": "",
        "claimed_at": null,
        "created_at": "2026-02-23T21:18:18.112426Z",
        "depends_on": [
          "fn-58-offline-skill-evaluation-framework-from.2",
          "fn-58-offline-skill-evaluation-framework-from.3"
        ],
        "epic": "fn-58-offline-skill-evaluation-framework-from",
        "id": "fn-58-offline-skill-evaluation-framework-from.4",
        "priority": null,
        "spec_path": ".flow/tasks/fn-58-offline-skill-evaluation-framework-from.4.md",
        "status": "todo",
        "title": "CI workflow, baseline regression, and documentation",
        "updated_at": "2026-02-23T21:19:56.898530Z"
      },
      "id": "fn-58-offline-skill-evaluation-framework-from.4",
      "runtime": null,
      "spec": "# fn-58-offline-skill-evaluation-framework-from.4 CI workflow, baseline regression, and documentation\n\n## Description\nAdd a GitHub Actions workflow for scheduled effectiveness evals, implement baseline regression tracking, and update all affected documentation.\n\n**Size:** M\n**Files:**\n- `.github/workflows/skill-evals.yml` (new workflow: weekly schedule + manual trigger)\n- `tests/evals/compare_baseline.py` (flesh out skeleton from task .1)\n- `tests/evals/baselines/effectiveness-baseline.json` (initial baseline after first run)\n- `CONTRIBUTING-SKILLS.md` (new section on skill evals in Section 5)\n- `AGENTS.md` (update file structure and validation commands)\n- `docs/skill-eval-framework.md` (new reference doc)\n- `README.md` (update testing section)\n- `CHANGELOG.md` (add entry under [Unreleased])\n\n## Approach\n\n- Workflow follows the pattern of `.github/workflows/agent-live-routing.yml:1` \u2014 weekly schedule + workflow_dispatch, artifact upload, secret-gated (skips if no API key)\n- Baseline comparison follows the pattern of `scripts/compare-agent-routing-baseline.py:1` \u2014 per-skill score tracking, regression detection (>0.5 point drop or >10% win rate drop), new-skill handling (no comparison for skills absent from baseline)\n- Doc updates follow existing patterns identified by docs-gap-scout:\n  - `CONTRIBUTING-SKILLS.md` Section 5: new subsection \"Skill Eval Cases\" with rubric authoring instructions\n  - `AGENTS.md` `## File Structure`: add `tests/evals/` entries; `## Validation Commands`: add eval commands\n  - `docs/skill-eval-framework.md`: follows layout of `docs/agent-routing-tests.md` (Files / Commands / Concepts)\n  - `README.md`: parallel paragraph in \"Agent Skill Routing Checks\" section\n  - `CHANGELOG.md`: `### Added` entry under `## [Unreleased]`\n\n## Key context\n\n- Memory pitfall: `GITHUB_BASE_REF` is empty for `workflow_dispatch` and `schedule` triggers. Baseline comparison must use a committed baseline file, not a git-ref-based approach, for scheduled runs.\n- Memory pitfall: In GHA steps with `set -euo pipefail`, capturing exit codes with `cmd; EXIT=$?` is dead code. Use `set +e` before the command.\n- Memory decision: New entries absent from the baseline should be treated as \"new coverage\" (no regression comparison), not hard failures.\n- The workflow should upload eval results as artifacts (following the pattern of `agent-live-routing.yml` artifact uploads).\n## Acceptance\n- [ ] `.github/workflows/skill-evals.yml` exists with weekly cron + workflow_dispatch triggers\n- [ ] Workflow skips gracefully when `ANTHROPIC_API_KEY` secret is not set (fork-safe)\n- [ ] Eval results uploaded as GitHub Actions artifacts\n- [ ] `compare_baseline.py` detects regressions (>0.5 point drop or >10% win rate drop per skill)\n- [ ] `compare_baseline.py` treats new skills (absent from baseline) as \"new coverage\", not failures\n- [ ] `CONTRIBUTING-SKILLS.md` Section 5 includes rubric authoring instructions and eval commands\n- [ ] `AGENTS.md` file structure and validation commands updated\n- [ ] `docs/skill-eval-framework.md` exists with Files/Commands/Concepts structure\n- [ ] `README.md` testing section references the eval framework\n- [ ] `CHANGELOG.md` has `### Added` entry under `## [Unreleased]`\n- [ ] `./scripts/validate-skills.sh && ./scripts/validate-marketplace.sh` still pass\n## Done summary\nTBD\n\n## Evidence\n- Commits:\n- Tests:\n- PRs:\n"
    },
    {
      "data": {
        "assignee": null,
        "claim_note": "",
        "claimed_at": null,
        "created_at": "2026-02-23T21:20:04.190239Z",
        "depends_on": [
          "fn-58-offline-skill-evaluation-framework-from.1"
        ],
        "epic": "fn-58-offline-skill-evaluation-framework-from",
        "id": "fn-58-offline-skill-evaluation-framework-from.5",
        "priority": null,
        "spec_path": ".flow/tasks/fn-58-offline-skill-evaluation-framework-from.5.md",
        "status": "todo",
        "title": "Offline activation eval dataset and runner",
        "updated_at": "2026-02-23T21:20:32.393877Z"
      },
      "id": "fn-58-offline-skill-evaluation-framework-from.5",
      "runtime": null,
      "spec": "# fn-58-offline-skill-evaluation-framework-from.5 Offline activation eval dataset and runner\n\n## Description\nBuild an offline activation eval that tests whether models correctly select skills given developer prompts \u2014 without requiring any CLI invocation. This complements the existing copilot smoke tests (29 cases, requires Copilot CLI) and live routing tests (20 cases, requires Claude/Codex/Copilot CLIs) by providing a fast, cheap, API-only activation eval.\n\nAdopt the JSONL dataset format and methodology from `dotnet-skills-evals/datasets/activation/`.\n\n**Size:** M\n**Files:**\n- `tests/evals/run_activation.py` (offline activation runner)\n- `tests/evals/datasets/activation/` (directory for JSONL test case files)\n- `tests/evals/datasets/activation/core_skills.jsonl` (test cases for core/high-traffic skills)\n- `tests/evals/datasets/activation/specialized_skills.jsonl` (test cases for specialized domain skills)\n- `tests/evals/datasets/activation/negative_controls.jsonl` (prompts that should NOT activate any target skill)\n\n## Approach\n\n- **Discovery mechanism under test**: Present the model with a system prompt containing a compressed skill routing index (skill names + descriptions) and a user prompt. Check if the model references the expected skill(s) in its response.\n- Follow the JSONL format from `dotnet-skills-evals`:\n  ```\n  {\"id\": \"act-001\", \"user_prompt\": \"...\", \"expected_skills\": [\"skill-name\"], \"acceptable_skills\": [], \"should_activate\": true, \"category\": \"domain\"}\n  ```\n- The runner reads skill descriptions from `skills/*/SKILL.md` frontmatter to build the compressed routing index dynamically (no hardcoded index)\n- Detection methods (layered, following cc-plugin-eval pattern):\n  1. **Programmatic**: Check if model explicitly names the skill (exact match in response text)\n  2. **LLM fallback**: If no exact match, ask a cheap model (Haiku) \"Did the response indicate skill X should be used?\"\n- Metrics tracked: TPR (true positive rate), FPR (false positive rate), accuracy, per-skill activation rate, token usage\n- Target: 50+ positive cases across 20+ skills, 15+ negative controls\n- Reuse test prompts from existing `tests/copilot-smoke/cases.jsonl` and `tests/agent-routing/cases.json` where applicable (convert to JSONL format)\n\n## Key context\n\n- The reference repo found that **tool-based discovery shows 100% activation including 100% FPR** \u2014 it measures tool-calling bias, not discovery quality. This is why the offline eval uses a compressed text index, not a tool-call mechanism.\n- The reference repo found that **short/generic skill names like `serialization` get false-positived** when models discuss the concept. Detection must filter for explicit skill references, not substring matches of domain concepts.\n- The reference repo's best result: **Sonnet + compressed index = 56.5% TPR, 0% FPR, 84.6% accuracy**. This is the baseline to beat with 131 skills.\n- Existing copilot smoke tests use `SKILL_LOAD_REGEX` and `SKILL_CALL_REGEX` patterns from `tests/copilot-smoke/run_smoke.py`. The offline runner should use similar detection patterns adapted for raw API responses.\n## Acceptance\n- [ ] `run_activation.py` runs offline activation eval using Anthropic API (no CLI required)\n- [ ] Compressed routing index built dynamically from `skills/*/SKILL.md` frontmatter descriptions\n- [ ] At least 50 positive activation test cases across 20+ skills in JSONL files\n- [ ] At least 15 negative control cases (prompts that should NOT activate target skills)\n- [ ] Detection uses layered approach: programmatic exact-match first, LLM fallback second\n- [ ] Metrics reported: TPR, FPR, accuracy, per-skill activation rate, total token usage, estimated cost\n- [ ] `--dry-run` mode shows planned cases without API calls\n- [ ] `--skill <name>` filters to cases for a specific skill\n- [ ] Results JSON output includes per-case details (prompt, expected, actual, detection_method, pass/fail)\n- [ ] Reuses at least 10 prompts adapted from existing `tests/copilot-smoke/cases.jsonl` or `tests/agent-routing/cases.json`\n## Done summary\nTBD\n\n## Evidence\n- Commits:\n- Tests:\n- PRs:\n"
    },
    {
      "data": {
        "assignee": null,
        "claim_note": "",
        "claimed_at": null,
        "created_at": "2026-02-23T21:20:04.309623Z",
        "depends_on": [
          "fn-58-offline-skill-evaluation-framework-from.1",
          "fn-58-offline-skill-evaluation-framework-from.3"
        ],
        "epic": "fn-58-offline-skill-evaluation-framework-from",
        "id": "fn-58-offline-skill-evaluation-framework-from.6",
        "priority": null,
        "spec_path": ".flow/tasks/fn-58-offline-skill-evaluation-framework-from.6.md",
        "status": "todo",
        "title": "Size impact and progressive disclosure evals",
        "updated_at": "2026-02-23T21:20:54.514169Z"
      },
      "id": "fn-58-offline-skill-evaluation-framework-from.6",
      "runtime": null,
      "spec": "# fn-58-offline-skill-evaluation-framework-from.6 Size impact and progressive disclosure evals\n\n## Description\nAdd size impact evals that test whether skill content format affects output quality: full SKILL.md vs truncated/summary-only vs no skill. This validates the progressive disclosure architecture and helps determine the optimal skill size for the 131-skill budget constraint.\n\n**Size:** M\n**Files:**\n- `tests/evals/run_size_impact.py` (size impact eval runner)\n- `tests/evals/datasets/size_impact/candidates.yaml` (skills selected for size testing, with truncation configs)\n\n## Approach\n\n- For each candidate skill, generate code under three conditions:\n  1. **Full**: Complete SKILL.md content injected as system prompt\n  2. **Summary**: Only frontmatter description + scope section (simulates compressed index)\n  3. **Baseline**: No skill content (generic .NET developer prompt)\n- Judge all three pairwise using the same rubric framework from task .3:\n  - Full vs Baseline (does the skill help at all?)\n  - Full vs Summary (does the full content add value over just the description?)\n  - Summary vs Baseline (does even a compressed version help?)\n- Select 8-10 candidate skills with varying sizes:\n  - Small skills (<2KB SKILL.md): test if they're already \"summary-sized\"\n  - Medium skills (2-5KB): the sweet spot\n  - Large skills (>5KB): test if progressive disclosure is needed\n  - Skills with sibling files: test SKILL.md-only vs SKILL.md + siblings\n- Reuse the A/B judging infrastructure from `run_effectiveness.py` (task .3)\n- Results feed into the description budget policy (WARN at 12,000 total, FAIL at 15,600)\n\n## Key context\n\n- The reference implementation found that progressive disclosure is the right solution for oversized skills \u2014 full content can push models toward over-engineered solutions.\n- dotnet-artisan already has the 15,600-char total description budget constraint. Size impact evals validate that this constraint is set at the right level.\n- Some skills reference sibling files (e.g., the sentinel test in `tests/copilot-smoke/fixture-plugin/`). The size impact eval should test SKILL.md-only injection for these skills to understand whether siblings add measurable value.\n- The `validate-skills.sh` already tracks `CURRENT_DESC_CHARS` and `PROJECTED_DESC_CHARS`. Size impact results could inform adjusting these thresholds.\n## Acceptance\n- [ ] `run_size_impact.py` generates code under Full, Summary, and Baseline conditions\n- [ ] At least 8 candidate skills selected spanning small, medium, and large sizes\n- [ ] Pairwise comparisons (Full vs Baseline, Full vs Summary, Summary vs Baseline) scored by LLM judge\n- [ ] Results JSON includes per-skill size tier, per-comparison scores, and winner\n- [ ] Summary report shows: which skills benefit from full content, which are effective as summary-only\n- [ ] At least 2 skills with sibling files tested with SKILL.md-only vs SKILL.md + siblings\n- [ ] `--dry-run` mode shows planned comparisons without API calls\n- [ ] Reuses rubric and judge infrastructure from `run_effectiveness.py`\n## Done summary\nTBD\n\n## Evidence\n- Commits:\n- Tests:\n- PRs:\n"
    },
    {
      "data": {
        "assignee": null,
        "claim_note": "",
        "claimed_at": null,
        "created_at": "2026-02-23T21:20:04.416735Z",
        "depends_on": [
          "fn-58-offline-skill-evaluation-framework-from.5"
        ],
        "epic": "fn-58-offline-skill-evaluation-framework-from",
        "id": "fn-58-offline-skill-evaluation-framework-from.7",
        "priority": null,
        "spec_path": ".flow/tasks/fn-58-offline-skill-evaluation-framework-from.7.md",
        "status": "todo",
        "title": "Expanded negative controls and confusion matrix tests",
        "updated_at": "2026-02-23T21:21:20.843045Z"
      },
      "id": "fn-58-offline-skill-evaluation-framework-from.7",
      "runtime": null,
      "spec": "# fn-58-offline-skill-evaluation-framework-from.7 Expanded negative controls and confusion matrix tests\n\n## Description\nExpand negative controls and build confusion matrix tests that validate skill disambiguation quality at scale. With 131 skills, overlapping descriptions are the primary misrouting risk. This task creates targeted test cases for skill groups that are most likely to be confused.\n\n**Size:** M\n**Files:**\n- `tests/evals/datasets/activation/confusion_matrix.jsonl` (prompts designed to test disambiguation)\n- `tests/evals/datasets/activation/negative_controls_expanded.jsonl` (additional negative controls)\n- `tests/evals/run_confusion_matrix.py` (confusion matrix analysis runner)\n- `tests/evals/reports/` (output directory for confusion matrix heatmaps/reports)\n\n## Approach\n\n- **Confusion matrix tests**: Group skills by domain overlap and create prompts that could plausibly match 2-3 skills. Measure which skill gets selected and whether it is the best one.\n  - Testing domain: `dotnet-xunit`, `dotnet-testing-strategy`, `dotnet-integration-testing`, `dotnet-snapshot-testing`, `dotnet-test-quality`\n  - Security domain: `dotnet-security-owasp`, `dotnet-api-security`, `dotnet-secrets-management`, `dotnet-cryptography`\n  - Data domain: `dotnet-efcore-patterns`, `dotnet-efcore-architecture`, `dotnet-data-access-strategy`\n  - Performance domain: `dotnet-performance-patterns`, `dotnet-benchmarkdotnet`, `dotnet-profiling`, `dotnet-gc-memory`\n  - API domain: `dotnet-minimal-apis`, `dotnet-api-versioning`, `dotnet-openapi`, `dotnet-input-validation`\n  - CI/CD domain: `dotnet-gha-patterns`, `dotnet-gha-build-test`, `dotnet-gha-publish`, `dotnet-gha-deploy`\n  - Blazor domain: `dotnet-blazor-patterns`, `dotnet-blazor-components`, `dotnet-blazor-auth`, `dotnet-blazor-testing`\n\n- **Expanded negative controls**: Add 15+ prompts for non-.NET topics (Python, Rust, Go, JavaScript, DevOps, general coding) that should NOT activate any dotnet-artisan skill. Also add \"temptation\" prompts that use .NET terminology in non-.NET contexts (e.g., \"What is the Observer pattern?\" \u2014 generic design pattern, not a .NET skill).\n\n- **Confusion matrix analysis**: For each domain group, build a confusion matrix showing activation rates across all group members. Flag:\n  - Cross-activation > 20% (skill B activates when skill A is expected)\n  - Low discrimination (multiple skills activate equally for the same prompt)\n  - Never-activated skills within a domain group\n\n- Follow memory pitfall: Negative control test cases must use temptation prompts that naturally overlap with the disallowed skill domain \u2014 prompts that avoid the domain will never trigger the skill, making the test case untestable.\n\n## Key context\n\n- The reference implementation focused on only 5 Akka.NET skills and already found confusion issues. With 131 skills across 21+ categories, confusion is exponentially more likely.\n- The existing similarity detection (`scripts/validate-similarity.py`) catches description overlap at the text level. Confusion matrix tests catch functional overlap \u2014 two skills with different descriptions that respond to the same prompts.\n- The reference implementation found that `serialization` (a short/generic name) was the primary false positive source. dotnet-artisan has several similarly generic names: `dotnet-containers`, `dotnet-validation-patterns`, `dotnet-channels`.\n## Acceptance\n- [ ] At least 7 domain groups defined with 3-5 skills each\n- [ ] At least 5 confusion matrix test prompts per domain group (35+ total)\n- [ ] At least 15 expanded negative control prompts (non-.NET + temptation prompts)\n- [ ] `run_confusion_matrix.py` produces per-group confusion matrices showing activation rates\n- [ ] Cross-activation > 20% flagged in report output\n- [ ] Low-discrimination cases (2+ skills activated equally) flagged in report\n- [ ] Results include per-prompt breakdown: prompt, expected_skill, actual_activations, pass/fail\n- [ ] `--group <name>` filters to a single domain group\n- [ ] Report output format is human-readable (Markdown table or similar)\n- [ ] At least one actionable finding about skill description improvement (demonstrated by results)\n## Done summary\nTBD\n\n## Evidence\n- Commits:\n- Tests:\n- PRs:\n"
    }
  ]
}
